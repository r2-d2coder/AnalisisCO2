# -*- coding: utf-8 -*-
"""AnalisisFinalCO2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PvvgftZqU8oRfxvQzB_P8Osi0-a4goSz

# An√°lisis de Resultados sobre la Generaci√≥n de CO‚ÇÇ en el Mundo

A continuaci√≥n, se detalla el **proceso de an√°lisis** y las actividades realizadas para dar respuesta a las 5 preguntas planteadas y obtener una conclusi√≥n sobre la emisi√≥n global de CO‚ÇÇ.

***

### Fuente de Datos

El conjunto de datos (dataset) se obtuvo de la p√°gina del **Banco Mundial** bajo el t√≠tulo "[CO‚ÇÇ and Greenhouse Gas Emissions](https://data360.worldbank.org/en/dataset/OWID_CB)". La metadata asociada, con la descripci√≥n de las variables, se encuentra disponible en el archivo [OWID_CB.pdf](https://drive.google.com/file/d/1xbqPqBR0ee_tF3udfTxCWpRGoFr0iXEj/view?usp=sharing).

***

'

'

'

#Pregunta 1: Adquisici√≥n y Preprocesamiento Integral de Datos (Question 1: Comprehensive Data Acquisition and Preprocessing)

Aqu√≠ comienza la respuesta detallada sobre la obtenci√≥n, limpieza y preparaci√≥n de los datos utilizados para el an√°lisis de las emisiones de $\text{CO}_2$.

---
"""

import pandas as pd

file_path = 'https://data360files.worldbank.org/data360-data/data/OWID_CB/OWID_CB.csv'

# Read the CSV file into a pandas DataFrame
try:
    df = pd.read_csv(file_path)
    # Display the first 5 rows of the DataFrame
    display(df.head())
except FileNotFoundError:
    print(f"Error: The file was not found at {file_path}. Please make sure the file exists in your Google Drive.")
except Exception as e:
    print(f"An error occurred: {e}")

"""## Paso 1: Exploraci√≥n Inicial de Datos y Verificaci√≥n de Errores üîç

Iniciaremos el **an√°lisis exploratorio** de los datos para identificar y diagnosticar posibles errores o inconsistencias en el *dataset*.

---

### Despliegue de Estructura

Como primer paso, se desplegar√°n los nombres de las **columnas** (variables) y el **tipo de dato** (`dtype`) que contiene cada una. Esta acci√≥n permite una inspecci√≥n preliminar de la estructura de la informaci√≥n, lo cual es vital antes de continuar con el procesamiento y an√°lisis.
"""

# Get information about the DataFrame, including data types and non-null values
display(df.info())

"""## Paso 1 (Continuaci√≥n): An√°lisis Descriptivo Inicial üìä

Una vez inspeccionada la estructura de los datos (columnas y tipos de datos), procedemos a realizar un **an√°lisis descriptivo** para obtener un resumen estad√≠stico b√°sico del *dataset*.

"""

import pandas as pd

# Remove the line that sets pandas display options to suppress scientific notation for integers
pd.options.display.float_format = '{:.0f}'.format

# Apply describe() to get descriptive statistics
display(df.describe())

# Reset pandas display options to default if needed later
# pd.reset_option('display.float_format')

"""## Paso 1 (Continuaci√≥n): An√°lisis de Valores Faltantes (Missing Values) üß©

Una vez revisada la estructura y las estad√≠sticas descriptivas, el siguiente paso cr√≠tico en el diagn√≥stico de la calidad de los datos es el **an√°lisis de los valores faltantes** (*missing values*).


"""

# Calculate missing values and their percentage
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100

# Create a DataFrame to display missing values and percentage
missing_df = pd.DataFrame({
    'Missing Count': missing_values,
    'Missing Percentage (%)': missing_percentage
})

# Display the DataFrame
display(missing_df)

"""## Paso 1 (Continuaci√≥n): Conteo y Frecuencia de Elementos √önicos üî¢

Una vez analizados los valores faltantes, el siguiente paso en la exploraci√≥n de datos es revisar la **variabilidad** de cada columna.

"""

# Display value counts for each column
for column in df.columns:
    print(f"Value counts for column: {column}")
    display(df[column].value_counts())
    print("\n" + "="*50 + "\n") # Separator for clarity

"""## Paso 2: Criterios y Ejecuci√≥n de Eliminaci√≥n de Columnas üóëÔ∏è

Tras completar la exhaustiva revisi√≥n de datos en el **Paso 3** (estructura, valores faltantes y frecuencias de elementos √∫nicos), se ha determinado que varias columnas no son √∫tiles o pertinentes para el an√°lisis de la generaci√≥n de $\text{CO}_2$.

---

### Justificaci√≥n para la Eliminaci√≥n

Se procede a **eliminar** las siguientes columnas del *dataset* por las razones que se detallan a continuaci√≥n:

* **Irrelevancia para el objetivo del an√°lisis:** Columnas que, a pesar de contener datos completos, no est√°n relacionadas con las preguntas clave sobre las emisiones de $\text{CO}_2$.
* **Baja Varianza / Etiqueta Constante:** Columnas cuya informaci√≥n es **siempre la misma** o presenta una varianza insignificante (ejemplo: una etiqueta id√©ntica en todos los registros). Estas columnas no aportan valor estad√≠stico ni discriminaci√≥n entre las filas.
* **Alto Porcentaje de Valores Faltantes:** Columnas que superan un umbral cr√≠tico de datos nulos, lo que comprometer√≠a la integridad de los resultados si se intentara utilizarlas o imputarlas.

### Ejecuci√≥n de la Limpieza

A continuaci√≥n, se presenta la lista espec√≠fica de columnas que se retirar√°n del *dataset* para asegurar que el an√°lisis se centre √∫nicamente en los **indicadores relevantes y de alta calidad**.
"""

# List of columns to drop
columns_to_drop = [
    'STRUCTURE',
    'STRUCTURE_ID',
    'ACTION',
    'FREQ',
    'FREQ_LABEL',
    'SEX',
    'SEX_LABEL',
    'AGE',
    'AGE_LABEL',
    'URBANISATION',
    'URBANISATION_LABEL',
    'COMP_BREAKDOWN_1',
    'COMP_BREAKDOWN_1_LABEL',
    'COMP_BREAKDOWN_2',
    'COMP_BREAKDOWN_2_LABEL',
    'COMP_BREAKDOWN_3',
    'COMP_BREAKDOWN_3_LABEL',
    'DATABASE_ID',
    'DATABASE_ID_LABEL',
    'UNIT_TYPE',
    'UNIT_TYPE_LABEL',
    'TIME_FORMAT',
    'TIME_FORMAT_LABEL',
    'COMMENT_OBS',
    'OBS_STATUS',
    'OBS_STATUS_LABEL',
    'OBS_CONF',
    'OBS_CONF_LABEL'
]

# Drop the specified columns from the DataFrame
df_filtered = df.drop(columns=columns_to_drop)

# Display the first few rows of the filtered DataFrame
display(df_filtered.head())

# Display information about the filtered DataFrame to see the remaining columns
display(df_filtered.info())

"""## Paso 2 (Continuaci√≥n): Valores Faltantes en `OBS_VALUE` üõ†Ô∏è

A diferencia de las columnas eliminadas previamente, se ha identificado que la falta de datos en la columna **`OBS_VALUE`** no se debe a un problema de irrelevancia o de alta cardinalidad, sino a la **ausencia de registros** para indicadores espec√≠ficos en ciertos a√±os o pa√≠ses.

"""

# Fill missing values in the 'OBS_VALUE' column with 0
df_filtered['OBS_VALUE'] = df_filtered['OBS_VALUE'].fillna(0)

# Verify that missing values in 'OBS_VALUE' have been filled
display(df_filtered['OBS_VALUE'].isnull().sum())

"""### Paso 2 (continuaci√≥n)
Hago un select de los primeros 20 valores para verificar la tabla
"""

# Set pandas display options to show all columns
pd.set_option('display.max_columns', None)

# Display the first 20 rows of the DataFrame
display(df_filtered.head(20))

# Reset pandas display options to default if needed later
# pd.reset_option('display.max_columns')

"""## Paso 3: Generaci√≥n de la Tabla Din√°mica (Pivot Table) üîÑ

El objetivo de este paso es **reestructurar los datos** para que la informaci√≥n clave sea m√°s intuitiva y f√°cil de analizar visualmente a lo largo del tiempo.

---

### Creaci√≥n de la Tabla Din√°mica

Se proceder√° a generar una **Tabla Din√°mica** (*Pivot Table*) utilizando el a√±o como eje de las columnas. Esta transformaci√≥n permite:

1.  **Agrupar** los datos por Pa√≠s y por Indicador.
2.  **Disponer** los a√±os como columnas separadas.

De esta forma, en una sola fila, se podr√° visualizar **el valor de un indicador espec√≠fico para un pa√≠s a lo largo de todos los a√±os disponibles**, facilitando la identificaci√≥n de tendencias y la comparaci√≥n temporal de los resultados.

"""

# Create a pivot table
pivot_table = df_filtered.pivot_table(
    index=['REF_AREA_LABEL', 'INDICATOR'],
    columns='TIME_PERIOD',
    values='OBS_VALUE',
    aggfunc='sum'
)

# Set pandas display options to show float values with a certain precision
pd.options.display.float_format = '{:.3f}'.format

# Display the pivot table
display(pivot_table)

# Reset pandas display options to default
pd.reset_option('display.float_format')

"""## Paso 3 (Continuaci√≥n): Segunda Tabla Din√°mica (Indicadores como Columnas) üîÑ

Se generar√° una **segunda Tabla Din√°mica** para obtener una vista alternativa de la informaci√≥n, enfocada en la comparaci√≥n simult√°nea de los indicadores dentro de un mismo a√±o.

---

### Reestructuraci√≥n de la Data

En esta nueva tabla, los **indicadores** (variables) se utilizar√°n como **columnas**. Esta disposici√≥n permite:

1.  **Agrupar** los datos por Pa√≠s y por A√±o.
2.  **Disponer** los **76 indicadores** como columnas separadas.

Esta reestructuraci√≥n facilita la **visualizaci√≥n transversal**, permitiendo analizar r√°pidamente **todos los valores de los indicadores** de un pa√≠s para un **a√±o espec√≠fico**. Es ideal para hacer comparaciones directas entre las distintas m√©tricas de $\text{CO}_2$ y otros factores por pa√≠s y por a√±o.

"""

# Create a second pivot table
pivot_table_2 = df_filtered.pivot_table(
    index=['REF_AREA_LABEL', 'TIME_PERIOD'],
    columns='INDICATOR',
    values='OBS_VALUE',
    aggfunc='sum'
)

# Display the second pivot table
display(pivot_table_2)

"""## Paso 4 : C√°lculo de la Matriz de Correlaci√≥n üìä

Una vez que los datos est√°n limpios y reestructurados, procedemos con el primer an√°lisis estad√≠stico: el c√°lculo de la **Matriz de Correlaci√≥n**.

---


Para este c√°lculo, utilizaremos la tabla **`pivote_table_2`** (la tabla din√°mica donde los indicadores son las columnas).

Esta tabla es la m√°s adecuada porque:

1.  **Estructura Ideal:** Al tener los **76 indicadores como columnas**, permite calcular la correlaci√≥n directa entre cada par de variables.
2.  **Visi√≥n Transversal:** Nos ofrece una visi√≥n de c√≥mo se relacionan todos los indicadores entre s√≠ dentro de un mismo punto de referencia (Pa√≠s y A√±o), esencial para identificar dependencias.

"""

# Calculate the correlation matrix
correlation_matrix = pivot_table_2.corr()

# Set pandas display options to show all rows and columns
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

# Display the correlation matrix
display(correlation_matrix)

# Reset pandas display options to default (optional, but good practice)
pd.reset_option('display.max_rows')
pd.reset_option('display.max_columns')

"""## Paso 4 (Continuaci√≥n): An√°lisis Detallado de Correlaciones üîç

Tras calcular la matriz de correlaci√≥n completa, se procede a un an√°lisis enfocado en los extremos para identificar las relaciones lineales m√°s fuertes.

---

### 1. Variables con Mayor Correlaci√≥n Positiva

Identificaremos los primeros pares de variables con el **valor de correlaci√≥n m√°s alto** (m√°s cercanos a $+1$) para confirmar las **relaciones esperadas** (como aquellas entre emisiones totales y sus fuentes, o entre variables de $\text{CO}_2$ y la temperatura acumulada) y detectar cualquier **redundancia** o acoplamiento de variables que deba ser considerado al modelar.


### 2. Variables con Menor Correlaci√≥n (o Mayor Correlaci√≥n Negativa)

Identificaremos los 50 pares de variables con el **valor de correlaci√≥n m√°s bajo** (cercanos a $0$ o a $-1$) para encontrar variables que se mueven en **direcci√≥n opuesta**. Por ejemplo, variables que aumentan a medida que las emisiones disminuyen (como la adopci√≥n de energ√≠as renovables o indicadores de eficiencia).
"""

# Unstack the correlation matrix to get a Series of correlation pairs
correlation_series = correlation_matrix.unstack()

# Drop the diagonal correlations (correlation of a variable with itself)
correlation_series = correlation_series[correlation_series.index.get_level_values(0) != correlation_series.index.get_level_values(1)]

# Drop duplicate correlation pairs (e.g., (A, B) and (B, A))
# We can do this by comparing the index tuples as sets or by ensuring a consistent order
# A simple way is to create a list of tuples and keep only one of each pair
unique_correlations = {}
for (ind1, ind2), value in correlation_series.items():
    # Create a consistent key by sorting the indicator names
    key = tuple(sorted((ind1, ind2)))
    # Add to the dictionary only if the key is not already present
    if key not in unique_correlations:
        unique_correlations[key] = value

# Convert the dictionary back to a pandas Series for sorting
unique_correlation_series = pd.Series(unique_correlations)


# Sort the unique correlation series in descending order
sorted_correlations = unique_correlation_series.sort_values(ascending=False)

# Display the first 50 sorted correlations
print("Primeras 50 correlaciones √∫nicas:")
display(sorted_correlations.head(50))

# Display the last 50 sorted correlations
print("\n√öltimas 50 correlaciones √∫nicas:")
display(sorted_correlations.tail(50))

"""## Paso 4 (Continuaci√≥n): An√°lisis de Correlaci√≥n con la Variable Clave (`OWID_CB_CO2`) üéØ

Para enfocarnos en las relaciones m√°s relevantes para el estudio, se a√≠sla la variable central de inter√©s (las emisiones totales de $\text{CO}_2$, `OWID_CB_CO2`) y se analizan las correlaciones m√°s fuertes y m√°s d√©biles que presenta con el resto de los indicadores.

---

### 1. Las 50 Mayores Correlaciones Positivas (Relaciones Fuertes)

### 2. Las 50 Menores Correlaciones (Relaciones D√©biles o Negativas)

"""

# Get the correlations for 'OWID_CB_CO2'
co2_correlations = correlation_matrix['OWID_CB_CO2']

# Drop the correlation of 'OWID_CB_CO2' with itself
co2_correlations = co2_correlations.drop('OWID_CB_CO2')

# Sort the correlations in descending order
sorted_co2_correlations = co2_correlations.sort_values(ascending=False)

# Display the top 50 correlations for 'OWID_CB_CO2'
print("Correlaciones de OWID_CB_CO2 con otras variables (Primeras 50):")
display(sorted_co2_correlations.head(50))
# Display the last 50 sorted correlations
print("\n√öltimas 50 correlaciones √∫nicas:")
display(sorted_co2_correlations.tail(50))

"""## Paso 4 (Continuaci√≥n): An√°lisis de Correlaci√≥n con la Variable Clave (`OWID_CB_GDP`) üéØ

Para enfocarnos en las relaciones m√°s relevantes para el estudio, se a√≠sla la variable central de inter√©s ( `OWID_CB_GDP`) y se analizan las correlaciones m√°s fuertes y m√°s d√©biles que presenta con el resto de los indicadores.

---

### 1. Las 50 Mayores Correlaciones Positivas (Relaciones Fuertes)

### 2. Las 50 Menores Correlaciones (Relaciones D√©biles o Negativas)
"""

# Get the correlations for 'OWID_CB_GDP'
gdp_correlations = correlation_matrix['OWID_CB_GDP']

# Drop the correlation of 'OWID_CB_GDP' with itself
gdp_correlations = gdp_correlations.drop('OWID_CB_GDP')

# Sort the correlations in descending order
sorted_gdp_correlations = gdp_correlations.sort_values(ascending=False)

# Display the top 50 correlations for 'OWID_CB_GDP'
print("Correlaciones de OWID_CB_GDP con otras variables (Primeras 50):")
display(sorted_gdp_correlations.head(50))

# Display the last 50 sorted correlations
print("\n√öltimas 50 correlaciones:")
display(sorted_gdp_correlations.tail(50))

"""## Paso 4 (Continuaci√≥n): An√°lisis de Correlaci√≥n con la Variable Clave (`OWID_CB_POPULATION`) üéØ

Para enfocarnos en las relaciones m√°s relevantes para el estudio, se a√≠sla la variable central de inter√©s ( `OWID_CB_POPULATION`) y se analizan las correlaciones m√°s fuertes y m√°s d√©biles que presenta con el resto de los indicadores.

---

### 1. Las 50 Mayores Correlaciones Positivas (Relaciones Fuertes)

### 2. Las 50 Menores Correlaciones (Relaciones D√©biles o Negativas)
"""

# Get the correlations for 'OWID_CB_POPULATION'
co2_per_POPULATION_correlations = correlation_matrix['OWID_CB_POPULATION']

# Drop the correlation of 'OWID_CB_POPULATION' with itself
co2_per_POPULATION_correlations = co2_per_POPULATION_correlations.drop('OWID_CB_POPULATION')

# Sort the correlations in descending order
sorted_co2_per_POPULATION_correlations = co2_per_POPULATION_correlations.sort_values(ascending=False)

# Display the top 50 correlations for 'OWID_CB_POPULATION'
print("Correlaciones de OWID_CB_POPULATION con otras variables (Primeras 50):")
display(co2_per_POPULATION_correlations.head(50))

# Display the last 50 sorted correlations
print("\n√öltimas 50 correlaciones:")
display(co2_per_POPULATION_correlations.tail(50))

# Get the correlations for 'OWID_CB_CO2_PER_CAPITA'
co2_per_capita_correlations = correlation_matrix['OWID_CB_CO2_PER_CAPITA']

# Drop the correlation of 'OWID_CB_CO2_PER_CAPITA' with itself
co2_per_capita_correlations = co2_per_capita_correlations.drop('OWID_CB_CO2_PER_CAPITA')

# Sort the correlations in descending order
sorted_co2_per_capita_correlations = co2_per_capita_correlations.sort_values(ascending=False)

# Display the top 50 correlations for 'OWID_CB_CO2_PER_CAPITA'
print("Correlaciones de OWID_CB_CO2_PER_CAPITA con otras variables (Primeras 50):")
display(sorted_co2_per_capita_correlations.head(50))

# Display the last 50 sorted correlations
print("\n√öltimas 50 correlaciones:")
display(sorted_co2_per_capita_correlations.tail(50))

"""## Paso 4 (Continuaci√≥n): An√°lisis de Correlaci√≥n con la Variable Clave (`OWID_CB_CO2_PER_GDP`) üéØ

Para enfocarnos en las relaciones m√°s relevantes para el estudio, se a√≠sla la variable central de inter√©s ( `OWID_CB_CO2_PER_GDP`) y se analizan las correlaciones m√°s fuertes y m√°s d√©biles que presenta con el resto de los indicadores.

---

### 1. Las 50 Mayores Correlaciones Positivas (Relaciones Fuertes)

### 2. Las 50 Menores Correlaciones (Relaciones D√©biles o Negativas)
"""

# Get the correlations for 'OWID_CB_CO2_PER_GDP '
co2_per_gdp_correlations = correlation_matrix['OWID_CB_CO2_PER_GDP']

# Drop the correlation of 'OWID_CB_CO2_PER_GDP' with itself
co2_per_gdp_correlations = co2_per_gdp_correlations.drop('OWID_CB_CO2_PER_GDP')

# Sort the correlations in descending order
sorted_co2_per_gdp_correlations = co2_per_gdp_correlations.sort_values(ascending=False)

# Display the top 50 correlations for 'OWID_CB_CO2_PER_GDP'
print("Correlaciones de OWID_CB_CO2_PER_GDP con otras variables (Primeras 50):")
display(sorted_co2_per_gdp_correlations.head(50))

# Display the last 50 sorted correlations
print("\n√öltimas 50 correlaciones:")
display(sorted_co2_per_gdp_correlations.tail(50))

"""## Paso 4 (Continuaci√≥n): An√°lisis de Correlaci√≥n con la Variable Clave (`OWID_CB_CUMULATIVE_CO2`) üéØ

Para enfocarnos en las relaciones m√°s relevantes para el estudio, se a√≠sla la variable central de inter√©s ( `OWID_CB_CUMULATIVE_CO2`) y se analizan las correlaciones m√°s fuertes y m√°s d√©biles que presenta con el resto de los indicadores.

---

### 1. Las 50 Mayores Correlaciones Positivas (Relaciones Fuertes)

### 2. Las 50 Menores Correlaciones (Relaciones D√©biles o Negativas)
"""

# Get the correlations for 'OWID_CB_CUMULATIVE_CO2'
cumulative_co2_correlations = correlation_matrix['OWID_CB_CUMULATIVE_CO2']

# Drop the correlation of 'OWID_CB_CUMULATIVE_CO2' with itself
cumulative_co2_correlations = cumulative_co2_correlations.drop('OWID_CB_CUMULATIVE_CO2')

# Sort the correlations in descending order
sorted_cumulative_co2_correlations = cumulative_co2_correlations.sort_values(ascending=False)

# Display the top 50 correlations for 'OWID_CB_CUMULATIVE_CO2'
print("Correlaciones de OWID_CB_CUMULATIVE_CO2 con otras variables (Primeras 50):")
display(sorted_cumulative_co2_correlations.head(50))

# Display the last 50 sorted correlations
print("\n√öltimas 50 correlaciones:")
display(sorted_cumulative_co2_correlations.tail(50))

"""## Paso 4 (Continuaci√≥n): An√°lisis de Correlaci√≥n con la Variable Clave (`OWID_CB_CUMULATIVE_CO2_INCLUDING_LUC`) üéØ

Para enfocarnos en las relaciones m√°s relevantes para el estudio, se a√≠sla la variable central de inter√©s ( `OWID_CB_CUMULATIVE_CO2_INCLUDING_LUC`) y se analizan las correlaciones m√°s fuertes y m√°s d√©biles que presenta con el resto de los indicadores.

---

### 1. Las 50 Mayores Correlaciones Positivas (Relaciones Fuertes)

### 2. Las 50 Menores Correlaciones (Relaciones D√©biles o Negativas)
"""

# Get the correlations for 'OWID_CB_CUMULATIVE_CO2_INCLUDING_LUC'
cumulative_co2_luc_correlations = correlation_matrix['OWID_CB_CUMULATIVE_CO2_INCLUDING_LUC']

# Drop the correlation of 'OWID_CB_CUMULATIVE_CO2_INCLUDING_LUC' with itself
cumulative_co2_luc_correlations = cumulative_co2_luc_correlations.drop('OWID_CB_CUMULATIVE_CO2_INCLUDING_LUC')

# Sort the correlations in descending order
sorted_cumulative_co2_luc_correlations = cumulative_co2_luc_correlations.sort_values(ascending=False)

# Display the top 50 correlations for 'OWID_CB_CUMULATIVE_CO2_INCLUDING_LUC'
print("Correlaciones de OWID_CB_CUMULATIVE_CO2_INCLUDING_LUC con otras variables (Primeras 50):")
display(sorted_cumulative_co2_luc_correlations.head(50))

# Display the last 50 sorted correlations
print("\n√öltimas 50 correlaciones:")
display(sorted_cumulative_co2_luc_correlations.tail(50))

"""## Paso 4 (Continuaci√≥n) : Interpretaci√≥n General de la Matriz de Correlaci√≥n üßê

## Conclusiones Principales del An√°lisis de Correlaci√≥n üßê

Bas√°ndome en el an√°lisis de todas las correlaciones que hemos obtenido, puedo indicar las siguientes conclusiones principales sobre las relaciones lineales entre los indicadores de emisiones de $\text{CO}_2$ y otras variables en este *dataset*:

---

### 1. Las Emisiones de $\text{CO}_2$ Est√°n Fuertemente Impulsadas por las Fuentes de Energ√≠a F√≥sil

Las **correlaciones extremadamente altas** entre `OWID_CB_CO2` y sus desgloses por fuente (`OWID_CB_COAL_CO2`, `OWID_CB_OIL_CO2`, `OWID_CB_GAS_CO2`, etc.) confirman que la quema de **combustibles f√≥siles** es el principal contribuyente a las emisiones totales. Esto es fundamental para cualquier estrategia de mitigaci√≥n: para reducir las emisiones totales, es portante eliminar la dependencia de estos combustibles.

### 2. Existe una Asociaci√≥n Hist√≥rica entre Desarrollo Socioecon√≥mico y Emisiones

Las **correlaciones positivas significativas** entre las emisiones de $\text{CO}_2$ (tanto absolutas como acumuladas) y variables como el **PIB** (`OWID_CB_GDP`) y **Poblaci√≥n** (`OWID_CB_POPULATION`) sugieren que, a lo largo del per√≠odo cubierto por los datos, el crecimiento econ√≥mico y el aumento de la poblaci√≥n han estado estrechamente ligados al incremento de las emisiones. Esto plantea el desaf√≠o de lograr el desarrollo econ√≥mico y social de una manera que est√© **desacoplada** de las altas emisiones.

### 3. Los Indicadores Normalizados Ofrecen una Perspectiva de Eficiencia

Analizar las correlaciones de indicadores como `OWID_CB_CO2_PER_CAPITA` y `OWID_CB_CO2_PER_GDP` nos da una idea de la **eficiencia** con la que se utiliza la energ√≠a en relaci√≥n con la poblaci√≥n o la producci√≥n econ√≥mica. Correlaciones negativas o d√©biles con estas variables normalizadas (en comparaci√≥n con las absolutas) podr√≠an indicar pa√≠ses que est√°n logrando un mayor crecimiento con un menor aumento proporcional de las emisiones, posiblemente a trav√©s de mejoras en la **eficiencia energ√©tica o cambios estructurales** en su econom√≠a.

### 4. La Acumulaci√≥n de $\text{CO}_2$ Se Correlaciona Fuertemente con el Cambio de Temperatura

La **alt√≠sima correlaci√≥n** entre las emisiones acumuladas de $\text{CO}_2$ (`OWID_CB_CUMULATIVE_CO2`, `OWID_CB_CUMULATIVE_CO2_INCLUDING_LUC`) y los indicadores de cambio de temperatura global (`OWID_CB_TEMPERATURE_CHANGE_FROM_CO2`, `OWID_CB_TEMPERATURE_CHANGE_FROM_GHG`) **refuerza la base cient√≠fica del cambio clim√°tico**. Esto demuestra claramente que la cantidad total de $\text{CO}_2$ liberada a la atm√≥sfera a lo largo de la historia est√° directamente relacionada con el calentamiento observado. La importancia de esta conclusi√≥n es inmensa, ya que valida la urgencia de reducir las emisiones acumuladas para limitar el calentamiento futuro.

### 5. Algunas Variables Muestran Correlaciones D√©biles o Negativas con las Emisiones

Las variables con correlaciones **cercanas a cero o negativas** (como `OWID_CB_TRADE_CO2` y `OWID_CB_TRADE_CO2_SHARE` con las emisiones totales, aunque esto puede variar seg√∫n el indicador espec√≠fico) pueden ser menos relevantes para explicar linealmente las emisiones totales o, en el caso de correlaciones negativas, podr√≠an estar relacionadas con factores que **contrarrestan** las emisiones (aunque la matriz de correlaci√≥n por s√≠ sola no puede confirmarlo sin un an√°lisis m√°s profundo).

---

### En Resumen

El an√°lisis de correlaci√≥n inicial confirma las **relaciones esperadas** entre las fuentes de energ√≠a, la actividad econ√≥mica, la poblaci√≥n y las emisiones de $\text{CO}_2$, y subraya la **fuerte conexi√≥n** entre las emisiones acumuladas y el cambio de temperatura global. Si bien la correlaci√≥n no demuestra causalidad, estos hallazgos son un punto de partida para comprender los principales factores asociados a la generaci√≥n de $\text{CO}_2$ y para identificar √°reas que merecen una investigaci√≥n m√°s profunda, posiblemente utilizando modelos m√°s avanzados que puedan explorar relaciones causales y no lineales.

'

'

'

#Pregunta 2: Modelado Predictivo y An√°lisis de Escenarios (Question 2: Predictive Modeling and Scenario Analysis) üîÆ

Aqu√≠ comienza la respuesta detallada sobre la **simulaci√≥n, construcci√≥n del modelo predictivo** y el **an√°lisis de escenarios** para las emisiones de $\text{CO}_2$.

## Paso 1: Preparaci√≥n del Dataset para Modelado üìä

En este paso, prepararemos el dataset para el entrenamiento del modelo predictivo. Seleccionaremos la variable objetivo (las emisiones de $\text{CO}_2$) y los indicadores que utilizaremos como variables predictoras.
"""

# The target variable is 'OWID_CB_CO2'
target_variable = 'OWID_CB_CO2'

# The features (predictor variables) will be all other indicator columns in pivot_table_2
# Exclude the target variable itself from the features
feature_variables = pivot_table_2.columns.drop(target_variable)

# Create the feature matrix (X) and the target vector (y)
X = pivot_table_2[feature_variables]
y = pivot_table_2[target_variable]

# Display the shapes of X and y to confirm
print("Shape of feature matrix (X):", X.shape)
print("Shape of target vector (y):", y.shape)

# Display the first few rows of X and y
print("\nFirst 5 rows of X:")
display(X.head())

print("\nFirst 5 rows of y:")
display(y.head())

"""## Paso 2: Divisi√≥n de Datos en Entrenamiento y Prueba üß±

Dividiremos el dataset preparado (X y y) en conjuntos de entrenamiento y prueba para el entrenamiento y la evaluaci√≥n del modelo predictivo. Utilizaremos un 80% de los datos para entrenar el modelo y el 20% restante para probar su rendimiento con datos no vistos.
"""

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting sets
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

"""# Paso 3: Modelo Predictivo
Desarrollar√© un modelo predictivo para pronosticar las emisiones de $\text{CO}_2$. Utilizar√© el conjunto completo de indicadores y probar√© 3 modelos de regresi√≥n, los evaluar√© y comparar√© para determinar cual es el mejor, posteriormente utilizar√© el mejor modelo para responder a la pregunta anal√≠tica: ¬´Si un pa√≠s aumenta su PIB en un 10 %, ¬øcu√°l es el cambio porcentual esperado en las emisiones de $\text{CO}_2$, suponiendo que todos los dem√°s factores se mantienen constantes?¬ª.

## Selecci√≥n y entrenamiento del primer modelo

Elegir y entrenar el primer modelo de regresi√≥n.

Modelo Linear Regression
"""

from sklearn.linear_model import LinearRegression

# Create a Linear Regression model instance
linear_reg_model = LinearRegression()

# Train the model using the training data
linear_reg_model.fit(X_train, y_train)

"""## Evaluaci√≥n del primer modelo


Evaluar el rendimiento del primer modelo utilizando el conjunto de prueba.

"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Make predictions on the test set
y_pred = linear_reg_model.predict(X_test)

# Calculate evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)  # Calculate RMSE by taking the square root of MSE
r2 = r2_score(y_test, y_pred)

# Print the evaluation metrics
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2) Score: {r2}")

"""## Selecci√≥n y entrenamiento del segundo modelo


Elegir y entrenar el segundo modelo de regresi√≥n.

Modelo Random Forest Regressor
"""

from sklearn.ensemble import RandomForestRegressor

# Instantiate a Random Forest Regressor model
random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the Random Forest Regressor model using the training data
random_forest_model.fit(X_train, y_train)

"""## Evaluaci√≥n del segundo modelo


Evaluar el rendimiento del segundo modelo utilizando el conjunto de prueba.

"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Make predictions on the test set using the trained Random Forest model
y_pred_rf = random_forest_model.predict(X_test)

# Calculate evaluation metrics for the Random Forest model
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)
r2_rf = r2_score(y_test, y_pred_rf)

# Print the evaluation metrics for the Random Forest model
print("Random Forest Regressor Model Evaluation:")
print(f"Mean Absolute Error (MAE): {mae_rf}")
print(f"Mean Squared Error (MSE): {mse_rf}")
print(f"Root Mean Squared Error (RMSE): {rmse_rf}")
print(f"R-squared (R2) Score: {r2_rf}")

"""## Selecci√≥n y entrenamiento del tercer modelo


Elegir y entrenar el tercer modelo de regresi√≥n.

Modelo GradientBoostingRegressor
"""

from sklearn.ensemble import GradientBoostingRegressor

# Instantiate a Gradient Boosting Regressor model with default parameters
gradient_boosting_model = GradientBoostingRegressor(random_state=42)

# Train the model using the training data
gradient_boosting_model.fit(X_train, y_train)

"""## Evaluaci√≥n del tercer modelo

Evaluar el rendimiento del tercer modelo utilizando el conjunto de prueba.

"""

# Make predictions on the test set using the trained Gradient Boosting model
y_pred_gb = gradient_boosting_model.predict(X_test)

# Calculate evaluation metrics for the Gradient Boosting model
mae_gb = mean_absolute_error(y_test, y_pred_gb)
mse_gb = mean_squared_error(y_test, y_pred_gb)
rmse_gb = np.sqrt(mse_gb)
r2_gb = r2_score(y_test, y_pred_gb)

# Print the evaluation metrics for the Gradient Boosting model
print("Gradient Boosting Regressor Model Evaluation:")
print(f"Mean Absolute Error (MAE): {mae_gb}")
print(f"Mean Squared Error (MSE): {mse_gb}")
print(f"Root Mean Squared Error (RMSE): {rmse_gb}")
print(f"R-squared (R2) Score: {r2_gb}")

"""## Paso 4: Comparaci√≥n de los Modelos üìäüìàüìâ

Se comparan las m√©tricas de evaluaci√≥n (MAE, MSE, RMSE, R¬≤) de los tres modelos de regresi√≥n entrenados (Linear Regression, Random Forest Regressor y Gradient Boosting Regressor) para determinar cu√°l tuvo el mejor rendimiento en el conjunto de prueba.

Bas√°ndonos en estas m√©tricas:

R2 Score: El modelo de Linear Regression tiene el R2 Score m√°s alto (m√°s cercano a 1), lo que indica que explica la mayor parte de la variabilidad en las emisiones de CO‚ÇÇ en el conjunto de prueba.
MAE (Mean Absolute Error): El modelo Random Forest Regressor tiene el MAE m√°s bajo, lo que significa que, en promedio, sus predicciones est√°n m√°s cerca de los valores reales.
MSE y RMSE (Mean Squared Error y Root Mean Squared Error): El modelo de Linear Regression tiene los valores m√°s bajos de MSE y RMSE, lo que sugiere que tiene errores de predicci√≥n m√°s peque√±os en general, especialmente penalizando menos los errores grandes en comparaci√≥n con los otros modelos en este caso.
Considerando que el R2 es muy alto para los tres modelos y que el MAE y RMSE son m√°s bajos para la Regresi√≥n Lineal, el modelo de Linear Regression parece ser el que tuvo el mejor rendimiento general en este conjunto de datos para la tarea de predicci√≥n.
"""

# Collect the evaluation metrics for each model
comparison_data = {
    'Model': ['Linear Regression', 'Random Forest Regressor', 'Gradient Boosting Regressor'],
    'MAE': [mae, mae_rf, mae_gb],
    'MSE': [mse, mse_rf, mse_gb],
    'RMSE': [rmse, rmse_rf, rmse_gb],
    'R2 Score': [r2, r2_rf, r2_gb]
}

# Create a pandas DataFrame for comparison
comparison_df = pd.DataFrame(comparison_data)

# Display the comparison table
print("Tabla Comparativa de M√©tricas de Evaluaci√≥n de Modelos:")
display(comparison_df)

# Determine the best model based on R2 (higher is better) or MAE/RMSE (lower is better)
# For this case, R2 is a good indicator of overall fit.
best_model_r2 = comparison_df.loc[comparison_df['R2 Score'].idxmax()]
best_model_mae = comparison_df.loc[comparison_df['MAE'].idxmin()]
best_model_rmse = comparison_df.loc[comparison_df['RMSE'].idxmin()]


print("\nMejor modelo basado en R2 Score:")
display(best_model_r2)

print("\nMejor modelo basado en MAE:")
display(best_model_mae)

print("\nMejor modelo basado en RMSE:")
display(best_model_rmse)

"""## Paso 5: Interpretaci√≥n del Mejor Modelo (Linear Regression) üßê

En este paso, interpretaremos los coeficientes del modelo de Regresi√≥n Lineal, que fue identificado como el de mejor rendimiento general. Los coeficientes nos indican la magnitud y direcci√≥n de la relaci√≥n lineal entre cada variable predictora y la variable objetivo (emisiones de $\text{CO}_2$), manteniendo otras variables constantes.

Dado que el modelo de Regresi√≥n Lineal tuvo el mejor rendimiento general, interpretaremos sus coeficientes para entender c√≥mo cada variable predictora influye en las emisiones de CO‚ÇÇ.

A continuaci√≥n se presentan los coeficientes del modelo de Regresi√≥n Lineal, ordenados por su valor absoluto.
"""

# Get the coefficients from the trained Linear Regression model
model_coefficients = linear_reg_model.coef_

# Create a pandas Series to associate coefficients with feature names
coefficients_series = pd.Series(model_coefficients, index=X_train.columns)

# Sort the coefficients by their absolute value to see the most influential features
sorted_coefficients = coefficients_series.abs().sort_values(ascending=False)

# Display the sorted coefficients (showing both positive and negative impacts)
print("Coeficientes del modelo de Regresi√≥n Lineal (ordenados por valor absoluto):")
pd.set_option('display.max_rows', None) # Set option to display all rows
pd.options.display.float_format = '{:.4f}'.format # Set float format to 4 decimal places
display(coefficients_series[sorted_coefficients.index]) # Display coefficients in the order of absolute value
pd.reset_option('display.max_rows') # Reset option to default
pd.reset_option('display.float_format') # Reset float format to default

"""## Paso 6: Simulaci√≥n de Escenario con Linear Regression - Aumento del 10% en el PIB üìà

En este paso, utilizaremos el modelo de Regresi√≥n Lineal (identificado como el mejor) para simular un escenario hipot√©tico donde el PIB (`OWID_CB_GDP`) aumenta en un 10%, manteniendo el resto de los indicadores constantes. Luego, predeciremos las emisiones de $\text{CO}_2$ en este escenario simulado y compararemos los resultados con las predicciones originales.
"""

# Combine training and testing data for simulation
X_combined = pd.concat([X_train, X_test])

# Make predictions on the original combined data using the Linear Regression model
y_pred_original_combined = linear_reg_model.predict(X_combined)

# Create a copy of the combined feature set for simulation
X_combined_simulated = X_combined.copy()

# Simulate a 10% increase in GDP ('OWID_CB_GDP') for all entries in the combined set
if 'OWID_CB_GDP' in X_combined_simulated.columns:
    X_combined_simulated['OWID_CB_GDP'] = X_combined_simulated['OWID_CB_GDP'] * 1.1
else:
    print("Warning: 'OWID_CB_GDP' column not found in the combined feature set.")

# Make predictions on the simulated combined set using the Linear Regression model
y_pred_simulated_combined = linear_reg_model.predict(X_combined_simulated)

# Calculate the difference in predicted CO2 emissions for the combined data
predicted_emission_change_combined = y_pred_simulated_combined - y_pred_original_combined

# Display the average predicted change in CO2 emissions for combined data
print(f"Average predicted change in CO2 emissions with 10% GDP increase (Combined Data - Linear Regression): {predicted_emission_change_combined.mean():.4f}")

# Display the total predicted change in CO2 emissions across combined data
print(f"Total predicted change in CO2 emissions across Combined Data with 10% GDP increase (Linear Regression): {predicted_emission_change_combined.sum():.4f}")

# Optionally, display the predicted change for the first few entries from combined data
#print("\nPredicted change in CO2 emissions for the first 10 entries (Combined Data - Linear Regression):")
#pd.options.display.float_format = '{:.6f}'.format # Ensure consistent formatting
#display(pd.Series(predicted_emission_change_combined).head(10))
#pd.reset_option('display.float_format')

# Visualize the distribution of the predicted changes for combined data
#plt.figure(figsize=(10, 6))
#sns.histplot(predicted_emission_change_combined, kde=True)
#plt.xlabel("Cambio Predicho en Emisiones de CO‚ÇÇ (Simulado - Original) - Datos Combinados")
#plt.ylabel("Frecuencia")
#plt.title("Distribuci√≥n del Cambio Predicho en Emisiones de CO‚ÇÇ con Aumento del 10% en PIB (Datos Combinados - Linear Regression)")
#plt.grid(True)
#plt.show()

"""# Conclusi√≥n


**Simulaci√≥n de Escenario (Aumento del 10% en el PIB):**

Utilizamos el mejor modelo (Regresion Lineal) para simular un escenario donde el PIB aumenta en un 10% para **todas las entradas en la combinaci√≥n de los conjuntos de entrenamiento y prueba**, manteniendo otros factores constantes. El modelo predijo una **disminuci√≥n promedio y total en las emisiones de $\text{CO}_2$** como resultado del incremento del 10% en el PIB. Esto cuantifica el impacto esperado del crecimiento econ√≥mico en las emisiones seg√∫n las relaciones lineales capturadas por el modelo en todo el rango de datos disponibles, indicando que, bajo las condiciones de esta simulaci√≥n, un aumento del PIB se asocia con una ligera reducci√≥n predicha en las emisiones.

'

'

'

#Pregunta 3: Problema de Fermi y an√°lisis de sensibilidad (Question 3: Fermi Problem and Sensitivity Analysis) üîÆ

Iniciar√© revisando el dataset existente (`pivot_table_2`) para identificar indicadores ya presentes que puedan ser relevantes para la adopci√≥n de VE y el transporte (ej. emisiones del transporte, consumo de energ√≠a per c√°pita, etc.). Si no hay indicadores directos de veh√≠culos o VE, esto se notar√° como una limitaci√≥n.
"""

# Examine the column names of the pivot_table_2 DataFrame
column_names = pivot_table_2.columns

# Define keywords related to electric vehicle adoption and transportation
keywords = ['transport', 'vehicle', 'energy consumption', 'electricity', 'oil', 'gas', 'coal']

# Identify relevant columns
relevant_columns = [col for col in column_names if any(keyword in col.lower() for keyword in keywords)]

# Display the identified relevant columns or note their absence
if relevant_columns:
    print("Relevant columns found in pivot_table_2:")
    for col in relevant_columns:
        print(f"- {col}")
else:
    print("No direct indicators related to electric vehicle adoption or transportation found in pivot_table_2.")
    print("This is a limitation for analyzing EV adoption directly using this dataset.")

"""## Paso 1: Estimar o simular un indicador de adopci√≥n de veh√≠culos el√©ctricos (VE)

Dado que es poco probable que tengamos datos directos de "n√∫mero de veh√≠culos per c√°pita" o "tasas actuales de adopci√≥n de VE" para todos los pa√≠ses y a√±os en el dataset, tendremos que pensar en como simular un escenario de "50% de la poblaci√≥n mundial adopta VE".  

**Nota:** Nuestro modelo actual (Linear Regression) se basa en relaciones lineales aprendidas de los datos existentes. Simular un cambio tan dr√°stico puede estar fuera del rango de los datos de entrenamiento y los resultados deben interpretarse con precauci√≥n.



"""

# Identify columns likely related to transportation emissions from the correlation analysis
# Based on the correlation analysis and general knowledge, oil and gas related emissions are most relevant
transportation_columns = [
    'OWID_CB_OIL_CO2',
    'OWID_CB_OIL_CO2_PER_CAPITA',
    'OWID_CB_GAS_CO2',
    'OWID_CB_GAS_CO2_PER_CAPITA',
    # Include other potentially related cumulative or share columns if desired,
    # but for a simplified direct impact, focusing on annual absolute/per capita might be sufficient initially.
    # Let's start with the main oil and gas emission columns:
]

# Ensure the identified columns are actually in the combined feature set
transportation_columns = [col for col in transportation_columns if col in X_combined.columns]

# Create a copy of the combined feature set for simulation
X_simulated_ev = X_combined.copy()

# Simulate a 50% reduction in the identified transportation-related CO2 emission columns
reduction_percentage = 0.50
for col in transportation_columns:
    X_simulated_ev[col] = X_simulated_ev[col] * (1 - reduction_percentage)

# Note the assumption made about the percentage reduction and the columns modified
print(f"Assumption: Simulated a {reduction_percentage*100}% reduction in the following columns:")
for col in transportation_columns:
    print(f"- {col}")
print("This simulates a shift towards electric vehicles, assuming a direct reduction in oil and gas CO2 emissions.")
print("This simulation is a simplification and may not capture the full complexity of EV adoption.")

# Display the first few rows of the simulated DataFrame to verify the changes
print("\nFirst 5 rows of X_simulated_ev after simulating EV adoption:")
display(X_simulated_ev.head())

"""## Paso 2: An√°lisis Predictivo de Escenarios: Impacto de la Adopci√≥n de Veh√≠culos El√©ctricos üöó

Bas√°ndonos en el modelo de regresi√≥n previamente entrenado, procederemos a realizar una **simulaci√≥n de escenario** para predecir el impacto de una r√°pida adopci√≥n de veh√≠culos el√©ctricos (VE) en las emisiones globales de $\text{CO}_2$.

Esta simulaci√≥n se logra mediante la **manipulaci√≥n directa de las variables de combustibles f√≥siles** m√°s relacionadas con el sector transporte.

---

### Metodolog√≠a de Simulaci√≥n (Escenario VE)

Para simular un alto grado de adopci√≥n de veh√≠culos el√©ctricos y la consecuente reducci√≥n en la dependencia de combustibles, aplicaremos una **reducci√≥n del 50%** a las siguientes variables clave en el *dataset* de predicci√≥n:

* **Variables de Emisiones de Petr√≥leo (Oil):**
    * `OWID_CB_OIL_CO2` (Emisiones totales de $\text{CO}_2$ de petr√≥leo)
    * `OWID_CB_OIL_CO2_PER_CAPITA` (Emisiones de $\text{CO}_2$ de petr√≥leo por persona)
* **Variables de Emisiones de Gas (Gas):**
    * `OWID_CB_GAS_CO2` (Emisiones totales de $\text{CO}_2$ de gas)
    * `OWID_CB_GAS_CO2_PER_CAPITA` (Emisiones de $\text{CO}_2$ de gas por persona)


"""

# Make predictions on the simulated combined data using the Linear Regression model
y_pred_simulated_ev = linear_reg_model.predict(X_simulated_ev)

# Get the original predictions for comparison (from the previous step)
# y_pred_original_combined is already available from the previous step

# Calculate the change in predicted CO2 emissions
predicted_emission_change_ev = y_pred_simulated_ev - y_pred_original_combined

# Calculate the percentage change in emissions.
# To avoid division by zero or very small numbers which can lead to skewed percentages,
# we can calculate the total change as a percentage of the total original predicted emissions.
total_original_predicted_emissions = y_pred_original_combined.sum()
total_predicted_emission_change_ev = predicted_emission_change_ev.sum()

# Calculate percentage change, handle case where original emissions are zero or close to zero
percentage_change_ev = 0
if total_original_predicted_emissions != 0:
    percentage_change_ev = (total_predicted_emission_change_ev / total_original_predicted_emissions) * 100

print(f"Total predicted change in CO2 emissions across Combined Data with simulated EV adoption (Linear Regression): {total_predicted_emission_change_ev:.4f}")
print(f"Percentage change in total predicted CO2 emissions with simulated EV adoption (Linear Regression): {percentage_change_ev:.4f}%")

# Display the average predicted change in CO2 emissions for combined data
print(f"Average predicted change in CO2 emissions with simulated EV adoption (Combined Data - Linear Regression): {predicted_emission_change_ev.mean():.4f}")

# Optionally, visualize the distribution of the predicted changes for combined data
# plt.figure(figsize=(10, 6))
# sns.histplot(predicted_emission_change_ev, kde=True)
# plt.xlabel("Cambio Predicho en Emisiones de CO‚ÇÇ (Simulado EV - Original) - Datos Combinados")
# plt.ylabel("Frecuencia")
# plt.title("Distribuci√≥n del Cambio Predicho en Emisiones de CO‚ÇÇ con Simulaci√≥n de Adopci√≥n de EV (Datos Combinados - Linear Regression)")
# plt.grid(True)
# plt.show()

"""## Paso 2 (Continuaci√≥n): Identificar pa√≠ses con la reducci√≥n m√°s significativa

Ordenar los pa√≠ses seg√∫n la magnitud de la reducci√≥n predicha en las emisiones de $\text{CO}_2$.

"""

# Create a pandas Series from the predicted emission changes, using the index of the combined features (country and year)
predicted_emission_change_series = pd.Series(predicted_emission_change_ev, index=X_combined.index)

# Group the series by the country (REF_AREA_LABEL) and sum the changes for each country
country_emission_changes = predicted_emission_change_series.groupby('REF_AREA_LABEL').sum()

# Drop the 'World' entry as it represents a global aggregate
if 'World' in country_emission_changes.index:
    country_emission_changes = country_emission_changes.drop('World')

# Sort the country-level emission changes in ascending order (most negative first for largest reductions)
sorted_country_emission_changes = country_emission_changes.sort_values(ascending=True)

# Display the top 20 sorted country-level emission changes
print("Predicted CO2 Emission Change by Country (Simulated EV Adoption), Sorted by Reduction (Top 20, excluding World):")
display(sorted_country_emission_changes.head(20))

"""## Resumen:

##Puntos importantes a considerar
###Supuestos y limitaciones:

###Supuestos:
- Se eligieron columnas espec√≠ficas relacionadas con las emisiones de petr√≥leo y gas como indicadores indirectos de las emisiones del transporte.
- Se aplic√≥ uniformemente una reducci√≥n del 50,0 % a estas columnas.
- Esto supone un impacto directo y proporcional en las emisiones de estas fuentes debido a la adopci√≥n de veh√≠culos el√©ctricos.

###Limitaciones:
- El modelo se basa en relaciones lineales hist√≥ricas y podr√≠a no reflejar los efectos complejos y no lineales de la adopci√≥n generalizada de veh√≠culos el√©ctricos.
- La simulaci√≥n es una simplificaci√≥n y no tiene en cuenta todos los factores reales que influyen en el impacto de la adopci√≥n de veh√≠culos el√©ctricos.
- El conjunto de datos carece de indicadores directos para la adopci√≥n de veh√≠culos el√©ctricos ni de m√©tricas detalladas de transporte.
- Interpretaci√≥n:
- Los resultados son predicciones basadas en estos supuestos simplificados y en las relaciones aprendidas del modelo, por lo que deben interpretarse con cautela.

### Hallazgos Clave del An√°lisis de Datos

* En el conjunto de datos se identificaron varios indicadores relacionados con las emisiones de CO2 del carb√≥n, el gas y el petr√≥leo, que sirven como indicadores indirectos del impacto en el transporte. Sin embargo, el conjunto de datos carece de indicadores directos para la adopci√≥n de veh√≠culos el√©ctricos o m√©tricas espec√≠ficas de transporte.
* Se utiliz√≥ una reducci√≥n simulada del 50 % en las columnas de emisiones de CO2 relacionadas con el petr√≥leo y el gas (`OWID_CB_OIL_CO2`, `OWID_CB_OIL_CO2_PER_CAPITA`, `OWID_CB_GAS_CO2`, `OWID_CB_GAS_CO2_PER_CAPITA`) para representar la transici√≥n hacia los veh√≠culos el√©ctricos.
* Seg√∫n el modelo de regresi√≥n lineal, se prev√© que esta reducci√≥n simulada del 50 % en las emisiones indirectas del transporte resulte en una reducci√≥n total de aproximadamente 171¬†245,63 unidades de emisiones de CO2 en todo el conjunto de datos. * Esta reducci√≥n prevista representa una disminuci√≥n promedio de aproximadamente 4,23 unidades por punto de datos (pa√≠s-a√±o) y un cambio porcentual de aproximadamente -4,78 % en las emisiones totales de CO2 previstas.
* El an√°lisis identific√≥ los pa√≠ses que se prev√© que experimenten las mayores reducciones de emisiones de CO2 en el escenario simulado, siendo Estados Unidos el que muestra la mayor reducci√≥n total, seguido de pa√≠ses como, la Federaci√≥n Rusa, China, Jap√≥n y Alemania.

### Perspectivas o pr√≥ximos pasos

* La simulaci√≥n sugiere que la reducci√≥n de las emisiones de fuentes de petr√≥leo y gas, que son indicadores indirectos del transporte, podr√≠a conducir a una disminuci√≥n notable de las emisiones totales de CO2, bas√°ndose en relaciones lineales hist√≥ricas.
* Los an√°lisis futuros deber√≠an incorporar indicadores m√°s directos de la adopci√≥n de veh√≠culos el√©ctricos y el consumo energ√©tico espec√≠fico del transporte, si est√°n disponibles, y explorar enfoques de modelado no lineal para captar mejor la compleja din√°mica de los cambios tecnol√≥gicos.

'

'

'

## Pregunta 4: Clasificaci√≥n e Implicaciones Pol√≠ticas (Question 4: Classification and Policy Implications)

Aqu√≠ comienza la respuesta detallada sobre la **construcci√≥n del modelo de clasificaci√≥n** para identificar pa√≠ses con probabilidades de lograr una reducci√≥n significativa de las emisiones de $\text{CO}_2$ en la pr√≥xima d√©cada, y la derivaci√≥n de **implicaciones pol√≠ticas**.

Para abordar esta pregunta, definimos una variable objetivo binaria clave: **`y_class`**.

### Definici√≥n de la Variable Objetivo Binaria (`y_class`)

La variable `y_class` se cre√≥ para representar si un pa√≠s tiene probabilidades de lograr una **reducci√≥n significativa de las emisiones de $\text{CO}_2$ en la pr√≥xima d√©cada**.

Se determin√≥ esta probabilidad analizando la **tendencia de las emisiones de $\text{CO}_2$ (`OWID_CB_CO2`) en los √∫ltimos 15 a√±os** disponibles en el conjunto de datos. Espec√≠ficamente, calculamos la pendiente de la l√≠nea de regresi√≥n lineal simple ajustada a las emisiones de $\text{CO}_2$ a lo largo del tiempo para cada pa√≠s.

*   Si la pendiente fue **negativa y menor que un umbral espec√≠fico** (indicando una tendencia clara a la baja en las emisiones), el pa√≠s se clasific√≥ como **1** (probable reducci√≥n significativa).
*   En otros casos (pendiente positiva, cercana a cero o negativa pero no por debajo del umbral), el pa√≠s se clasific√≥ como **0** (no probable reducci√≥n significativa).

##Paso 1: Es muy importante agregar a nuestro modelo informaci√≥n extra relacionada con indicadores de energ√≠a renovable
"""

import pandas as pd
import requests

# URL of the Excel file
excel_url = "https://api.worldbank.org/v2/es/indicator/EG.FEC.RNEW.ZS?downloadformat=excel"

# Download the Excel file
response = requests.get(excel_url)

# Save the file temporarily
excel_file_path = "/tmp/renewable_energy_consumption.xls"
with open(excel_file_path, 'wb') as f:
    f.write(response.content)

# Read the Excel file into a pandas DataFrame
# World Bank Excel files often have metadata at the beginning, so we might need to skip rows
# Let's try reading and inspect to determine the correct rows to skip
try:
    # Attempt to read, skipping initial rows that look like metadata
    renewable_energy_df = pd.read_excel(excel_file_path, skiprows=3) # Adjust skiprows if necessary after inspection
    print("Successfully read the Excel file.")
    print("\nFirst 5 rows:")
    display(renewable_energy_df.head())
    print("\nInfo on the DataFrame:")
    renewable_energy_df.info()

except Exception as e:
    print(f"Error reading Excel file: {e}")
    print("Please check the Excel file structure and adjust skiprows if needed.")

"""##Paso 1 (continuaci√≥n) Extraer las columnas que nos interesan"""

# Melt the DataFrame from wide to long format
# Identify identifier variables (Country Name, Country Code, Indicator Name, Indicator Code)
id_vars = ['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code']

# Identify value variables (the years) - assuming all columns after id_vars are years
value_vars = [col for col in renewable_energy_df.columns if col not in id_vars]

renewable_energy_long = renewable_energy_df.melt(
    id_vars=id_vars,
    value_vars=value_vars,
    var_name='TIME_PERIOD',
    value_name='OBS_VALUE'
)

# Rename columns to match the structure of our original data if necessary
# We might need to map 'Country Name' to 'REF_AREA_LABEL' or use 'Country Code'
# For simplicity, let's keep the original names for now and address mapping later if needed
# But let's rename the value column to something more descriptive for clarity
renewable_energy_long = renewable_energy_long.rename(columns={'OBS_VALUE': 'Renewable_Energy_Consumption_Percent'})

# Display the first few rows of the transformed DataFrame
print("Transformed Renewable Energy DataFrame (long format):")
display(renewable_energy_long.head())

# Display info to check data types and non-null counts
print("\nInfo on the transformed DataFrame:")
renewable_energy_long.info()

"""##Paso 1 (continuaci√≥n) Agregarlas a nuestra tabla pivote 2"""

# Reset the index of pivot_table_2 to make REF_AREA_LABEL and TIME_PERIOD columns for merging
pivot_table_2_reset = pivot_table_2.reset_index()

# Rename columns in the renewable energy dataframe to match pivot_table_2 for merging
# Assuming 'Country Name' in renewable_energy_long corresponds to 'REF_AREA_LABEL' in pivot_table_2
# And 'TIME_PERIOD' is already named the same
renewable_energy_long_renamed = renewable_energy_long.rename(columns={'Country Name': 'REF_AREA_LABEL'})

# Convert the 'TIME_PERIOD' column in renewable_energy_long_renamed to numeric
# Use errors='coerce' to turn any values that cannot be converted into NaN
renewable_energy_long_renamed['TIME_PERIOD'] = pd.to_numeric(renewable_energy_long_renamed['TIME_PERIOD'], errors='coerce')

# Drop rows where TIME_PERIOD could not be converted (if any)
renewable_energy_long_renamed.dropna(subset=['TIME_PERIOD'], inplace=True)

# Ensure TIME_PERIOD in both dataframes has the same integer type for merging
pivot_table_2_reset['TIME_PERIOD'] = pivot_table_2_reset['TIME_PERIOD'].astype(int)
renewable_energy_long_renamed['TIME_PERIOD'] = renewable_energy_long_renamed['TIME_PERIOD'].astype(int)


# Merge the two dataframes
# Use a left merge with pivot_table_2 as the base, to keep all rows from pivot_table_2
# and add the renewable energy data where available.
merged_df = pd.merge(
    pivot_table_2_reset,
    renewable_energy_long_renamed[['REF_AREA_LABEL', 'TIME_PERIOD', 'Renewable_Energy_Consumption_Percent']],
    on=['REF_AREA_LABEL', 'TIME_PERIOD'],
    how='left'
)

# Set the index back to REF_AREA_LABEL and TIME_PERIOD
merged_df = merged_df.set_index(['REF_AREA_LABEL', 'TIME_PERIOD'])

# Fill NaN values in the newly added 'Renewable_Energy_Consumption_Percent' column with 0
merged_df['Renewable_Energy_Consumption_Percent'] = merged_df['Renewable_Energy_Consumption_Percent'].fillna(0)

# Display the first few rows and info of the merged dataframe
print("Merged DataFrame with Renewable Energy Consumption:")
display(merged_df.head())
print("\nInfo on the merged DataFrame:")
merged_df.info()

"""##Paso 1 (continuaci√≥n) Agregar m√°s indicadores de energ√≠a sustentable"""

import pandas as pd
import requests

# URL of the CSV file
csv_url = "https://data360files.worldbank.org/data360-data/data/WB_RISE/WB_RISE.csv"

# Download the CSV file
response = requests.get(csv_url)
response.raise_for_status() # Raise an exception for bad status codes

# Read the CSV file into a pandas DataFrame
# The file is likely in UTF-8 encoding, but sometimes other encodings are used.
# Let's try reading directly and handle potential encoding errors if they occur.
try:
    rise_df = pd.read_csv(csv_url)
    print("Successfully read the CSV file.")
    # Select only the requested columns
    rise_df_filtered = rise_df[['REF_AREA', 'REF_AREA_LABEL', 'TIME_PERIOD', 'OBS_VALUE', 'INDICATOR', 'INDICATOR_LABEL']]
    print("\nFirst 5 rows of filtered data:")
    display(rise_df_filtered.head())
    print("\nInfo on the filtered DataFrame:")
    rise_df_filtered.info()


except Exception as e:
    print(f"Error reading CSV file: {e}")
    print("Attempting to read with a different encoding (e.g., 'latin1')...")
    try:
        rise_df = pd.read_csv(csv_url, encoding='latin1')
        print("Successfully read the CSV file with 'latin1' encoding.")
         # Select only the requested columns
        rise_df_filtered = rise_df[['REF_AREA', 'REF_AREA_LABEL', 'TIME_PERIOD', 'OBS_VALUE', 'INDICATOR', 'INDICATOR_LABEL']]
        print("\nFirst 5 rows of filtered data:")
        display(rise_df_filtered.head())
        print("\nInfo on the filtered DataFrame:")
        rise_df_filtered.info()
    except Exception as e2:
        print(f"Error reading CSV file with 'latin1' encoding: {e2}")
        print("Could not read the CSV file with common encodings. Please check the file.")

"""##Paso 1 (continuaci√≥n) Agregarlos a la tabla pivote 2"""

# Create a pivot table from the filtered RISE data
# Use REF_AREA_LABEL and TIME_PERIOD as index, INDICATOR as columns, and OBS_VALUE as values
rise_pivot = rise_df_filtered.pivot_table(
    index=['REF_AREA_LABEL', 'TIME_PERIOD'],
    columns='INDICATOR',
    values='OBS_VALUE',
    aggfunc='first' # Use 'first' or another appropriate aggregation if there are multiple values for the same indicator/country/year
)

# Display the first few rows of the pivoted RISE DataFrame
print("Pivoted RISE DataFrame:")
display(rise_pivot.head())

# Display info on the pivoted RISE DataFrame
print("\nInfo on the pivoted RISE DataFrame:")
rise_pivot.info()

"""##Paso 1 (continuaci√≥n) Integraci√≥n de la informaci√≥n"""

# Merge the merged_df (original data + renewable energy) with the pivoted RISE data
# Use a left merge with merged_df as the base to keep all rows from the original data
# and add the RISE indicators where available.
final_merged_df = pd.merge(
    merged_df.reset_index(), # Reset index for merging
    rise_pivot.reset_index(), # Reset index for merging
    on=['REF_AREA_LABEL', 'TIME_PERIOD'],
    how='left'
)

# Set the index back to REF_AREA_LABEL and TIME_PERIOD
final_merged_df = final_merged_df.set_index(['REF_AREA_LABEL', 'TIME_PERIOD'])

# Display the first few rows and info of the final merged dataframe
print("Final Merged DataFrame with Original, Renewable Energy, and RISE Data:")
display(final_merged_df.head())
print("\nInfo on the final merged DataFrame:")
final_merged_df.info()

"""##Paso 1 (continuaci√≥n) agregar m√°s informaci√≥n energ√©tica relevante sobre consumo de energ√≠a limpia"""

import pandas as pd
import requests

# URL of the new CSV file
csv_url_se4all = "https://data360files.worldbank.org/data360-data/data/WB_SE4ALL/WB_SE4ALL_EG_FCON_RNEW.csv"

# Download the CSV file
response_se4all = requests.get(csv_url_se4all)
response_se4all.raise_for_status() # Raise an exception for bad status codes

# Read the CSV file into a pandas DataFrame
try:
    se4all_df = pd.read_csv(csv_url_se4all)
    print("Successfully read the new SE4ALL CSV file.")
    print("\nFirst 5 rows:")
    display(se4all_df.head())
    print("\nInfo on the DataFrame:")
    se4all_df.info()

except Exception as e:
    print(f"Error reading the new SE4ALL CSV file: {e}")
    print("Attempting to read with a different encoding (e.g., 'latin1')...")
    try:
        se4all_df = pd.read_csv(csv_url_se4all, encoding='latin1')
        print("Successfully read the new SE4ALL CSV file with 'latin1' encoding.")
        print("\nFirst 5 rows:")
        display(se4all_df.head())
        print("\nInfo on the DataFrame:")
        se4all_df.info()
    except Exception as e2:
        print(f"Error reading the new SE4ALL CSV file with 'latin1' encoding: {e2}")
        print("Could not read the new SE4ALL CSV file with common encodings. Please check the file.")

"""##Paso 1 (continuaci√≥n) seleccionar las columnas necesarias"""

# Filter the se4all_df to get only the 'WB_SE4ALL_EG_FCON_RNEW' indicator
se4all_renewable_consumption = se4all_df[se4all_df['INDICATOR'] == 'WB_SE4ALL_EG_FCON_RNEW'].copy()

# Select relevant columns and pivot the data
se4all_renewable_pivot = se4all_renewable_consumption.pivot_table(
    index=['REF_AREA_LABEL', 'TIME_PERIOD'],
    columns='INDICATOR',
    values='OBS_VALUE',
    aggfunc='sum' # Use sum in case there are multiple entries for a country/year/indicator
)

# Rename the column for clarity
se4all_renewable_pivot = se4all_renewable_pivot.rename(columns={'WB_SE4ALL_EG_FCON_RNEW': 'Renewable_Energy_Consumption_PJ'})

# Display the first few rows and info of the pivoted SE4ALL DataFrame
print("Pivoted SE4ALL Renewable Energy Consumption (PJ):")
display(se4all_renewable_pivot.head())
print("\nInfo on the pivoted SE4ALL DataFrame:")
se4all_renewable_pivot.info()

"""##Paso 1 (continuaci√≥n) Realizar transformaciones pivote para que se pueda integrar la informaci√≥n"""

# Merge the final_merged_df with the pivoted SE4ALL renewable consumption data
# Use a left merge with final_merged_df as the base
final_merged_df = pd.merge(
    final_merged_df.reset_index(), # Reset index for merging
    se4all_renewable_pivot.reset_index(), # Reset index for merging
    on=['REF_AREA_LABEL', 'TIME_PERIOD'],
    how='left'
)

# Set the index back to REF_AREA_LABEL and TIME_PERIOD
final_merged_df = final_merged_df.set_index(['REF_AREA_LABEL', 'TIME_PERIOD'])

# Fill NaN values in the newly added 'Renewable_Energy_Consumption_PJ' column with 0 (or another appropriate strategy)
# Given the user's previous preference, filling with 0 might be suitable, but consider if NaN should represent missing data.
# For now, let's fill with 0 as per previous requests.
final_merged_df['Renewable_Energy_Consumption_PJ'] = final_merged_df['Renewable_Energy_Consumption_PJ'].fillna(0)

# Display the first few rows and info of the final merged dataframe
print("Final Merged DataFrame with Original, Renewable Energy (%), RISE, and SE4ALL (PJ) Data:")
display(final_merged_df.head())
print("\nInfo on the final merged DataFrame:")
final_merged_df.info()

"""**Paso 2** Aqu√≠ se tiene como objetivo principal preparar una variable objetivo binaria para un modelo de clasificaci√≥n, bas√°ndose en la tendencia hist√≥rica de las emisiones de CO2 de cada pa√≠s.

Se responde a la pregunta ¬øHa logrado un pa√≠s una reducci√≥n significativa y constante de sus emisiones en los √∫ltimos 15 a√±os?
"""

from scipy.stats import linregress
import numpy as np

# Define the time window for trend analysis (e.g., last 15 years available in the data)
# Let's find the maximum year in the dataset
max_year = final_merged_df.index.get_level_values('TIME_PERIOD').max()
min_year_for_trend = max_year - 14 # Look at the last 15 years (inclusive of max_year)

# Filter the dataframe to include only data within the trend analysis window
recent_data = final_merged_df.loc[(slice(None), slice(min_year_for_trend, max_year)), 'OWID_CB_CO2'].dropna()

# Group the data by country and calculate the trend (slope) for each country
emission_trends = {}
for country in recent_data.index.get_level_values('REF_AREA_LABEL').unique():
    country_data = recent_data.loc[country]
    if len(country_data) >= 5: # Require at least 5 data points to calculate a meaningful trend
        years = country_data.index.get_level_values('TIME_PERIOD').values
        emissions = country_data.values

        # Perform linear regression to get the slope
        slope, intercept, r_value, p_value, std_err = linregress(years, emissions)
        emission_trends[country] = slope
    # else: # Countries with less than 5 data points will not be included in the binary target

# Convert the emission trends to a pandas Series
emission_trends_series = pd.Series(emission_trends)

# Define the binary target variable based on the emission trend (slope)
# A negative slope indicates a decreasing trend.
# Let's define "significant reduction" as having a negative slope below a certain threshold
# The threshold value might need adjustment based on the distribution of slopes
slope_threshold = -0.5 # Example threshold: slope less than -0.5 (significant decrease per year)

# Create the binary target: 1 if significant reduction (slope < threshold), 0 otherwise
binary_target = (emission_trends_series < slope_threshold).astype(int)

# Display the first few rows of the emission trends and binary target
print(f"Emission trends (slope of CO2 vs. Year) for the last 15 years:")
display(emission_trends_series.head())

print(f"\nBinary target (Significant Reduction: slope < {slope_threshold}):")
display(binary_target.head())

# Display the distribution of the binary target
print("\nDistribution of the binary target:")
display(binary_target.value_counts())

# Display the number of countries included in this target definition
print(f"\nNumber of countries included in the binary target definition: {len(binary_target)}")

"""**Paso 2 (continuaci√≥n)**: Se prepara el dataset para el Modelado de Clasificaci√≥n, cuyo objetivo es predecir qu√© caracter√≠sticas (variables) hacen probable que un pa√≠s logre una reducci√≥n significativa de sus emisiones de CO2 (la variable binaria binary_target que se obtuvo arriba).
El proceso se enfoca en seleccionar las caracter√≠sticas (X) correctas, alinear los datos temporales y asegurar que las filas de las caracter√≠sticas (X_class_aligned) y la variable objetivo (y_class) coincidan perfectamente.
"""

# We need to select features (X) and the binary target (y_class)
# The features will be the columns from final_merged_df, excluding the original CO2 emission variable
# and potentially other variables that are direct components or highly correlated with the target definition process.
# For simplicity, let's start by excluding the original 'OWID_CB_CO2' column and the cumulative CO2 columns
# as the trend in CO2 is our target.

# Identify columns to exclude from features
# Exclude the original CO2 variable as the target is derived from its trend
# Exclude cumulative CO2 variables as they are highly related to the trend over time
# Exclude the original index columns if they were reset
columns_to_exclude = ['OWID_CB_CO2', 'OWID_CB_CUMULATIVE_CO2', 'OWID_CB_CUMULATIVE_CO2_INCLUDING_LUC']
# Also exclude the target variable if it was added as a column (it's a separate Series here)


# Create the feature set (X_class) by dropping excluded columns from final_merged_df
# We need to select the data for the countries that are included in our binary_target
countries_with_target = binary_target.index
# Filter final_merged_df to include only the countries that have a binary target defined
filtered_df_for_classification = final_merged_df.loc[countries_with_target].copy() # Use .copy() to avoid SettingWithCopyWarning

# Select features, excluding the specified columns
X_class = filtered_df_for_classification.drop(columns=columns_to_exclude, errors='ignore')

# For classification, we typically use features from a specific point in time, or aggregates over time.
# Given the target is based on a trend up to the max year, let's use features from the latest year available for each country.

# Sort the filtered data by country and time period to easily get the last entry for each country
X_class_sorted = X_class.sort_index(level=['REF_AREA_LABEL', 'TIME_PERIOD'])

# Get the latest entry (latest year) for each country using groupby() and tail(1)
X_class_latest = X_class_sorted.groupby('REF_AREA_LABEL').tail(1).copy() # Use .copy() after tail(1)

# Ensure the index of X_class_latest is just the country name for alignment with binary_target
X_class_latest = X_class_latest.reset_index(level='TIME_PERIOD', drop=True)

# Align X_class_latest with binary_target based on the country index
# This step should now work as both have a unique country index
X_class_aligned = X_class_latest.loc[binary_target.index]


# The target variable is binary_target
y_class = binary_target

# Display the shapes of X_class_aligned and y_class to confirm
print("Shape of feature matrix for classification (X_class_aligned):", X_class_aligned.shape)
print("Shape of target vector for classification (y_class):", y_class.shape)

# Display the first few rows of X_class_aligned and y_class
print("\nFirst 5 rows of X_class_aligned:")
display(X_class_aligned.head())

print("\nFirst 5 rows of y_class:")
display(y_class.head())

# Check for any remaining NaN values in the feature set and decide how to handle them
print("\nChecking for NaN values in the feature set:")
display(X_class_aligned.isnull().sum()[X_class_aligned.isnull().sum() > 0])

# Note: Handling remaining NaNs might be necessary before training the model.
# Common strategies include imputation (mean, median, mode) or dropping columns/rows.

"""**Paso 2 (continuaci√≥n)**: Esta variable `y_class` sirve como nuestra **variable objetivo binaria** para el modelo de clasificaci√≥n. El objetivo del clasificador es aprender a predecir esta variable (`y_class`) bas√°ndose en el resto de los indicadores disponibles en el dataset, lo que nos permitir√° identificar qu√© caracter√≠sticas est√°n asociadas con los pa√≠ses que han mostrado una tendencia a la reducci√≥n de emisiones."""

# Check the value counts of the binary target variable
print("Value counts of the binary target variable:")
display(y_class.value_counts())

# Check the proportion of each class in the binary target variable
print("\nProportion of each class in the binary target variable:")
display(y_class.value_counts(normalize=True))

"""**Paso 2 (Continuaci√≥n)**: Imputaci√≥n de Valores Faltantes en las Caracter√≠sticas

Antes de entrenar un modelo de clasificaci√≥n, es necesario manejar los valores faltantes en el conjunto de caracter√≠sticas (`X_class`), ya que la mayor√≠a de los algoritmos de *machine learning* no pueden procesar datos con valores nulos.

Esta celda de c√≥digo realiza la imputaci√≥n de valores faltantes utilizando la **mediana** de cada columna. La mediana es una opci√≥n robusta para la imputaci√≥n, ya que es menos sensible a valores at√≠picos (outliers) en comparaci√≥n con la media.

El resultado es un nuevo DataFrame, `X_class_imputed`, donde todos los valores faltantes han sido reemplazados por la mediana de sus respectivas columnas. La verificaci√≥n final confirma que no quedan valores nulos.
"""

# Impute missing values in the feature set (X_class) using the median
# Calculate the median for each column *before* imputation
medians = X_class.median()

# Fill missing values with the calculated medians
X_class_imputed = X_class.fillna(medians)

# Verify that there are no more missing values in the imputed feature set
print("Missing values in X_class_imputed after imputation:")
display(X_class_imputed.isnull().sum().sum())

"""## Paso 2: Divisi√≥n de Datos en Entrenamiento y Prueba para Clasificaci√≥n üß±

Preparamos los datos para el entrenamiento y la evaluaci√≥n del modelo de clasificaci√≥n. Dado que nuestra variable objetivo binaria (`y_class`) tiene una entrada por pa√≠s (basada en la tendencia de emisiones a lo largo del tiempo), necesitamos alinear nuestro conjunto de caracter√≠sticas (`X_class_imputed`), que inicialmente tiene m√∫ltiples entradas por pa√≠s (una por a√±o), para que tambi√©n tenga una entrada por pa√≠s.

La celda de c√≥digo realiza lo siguiente:

1.  Selecciona los datos del **a√±o m√°s reciente disponible** para cada pa√≠s en el conjunto de caracter√≠sticas imputado (`X_class_imputed`). Esto crea un DataFrame (`X_class_latest`) con una fila por pa√≠s.
2.  Asegura que el √≠ndice de este nuevo DataFrame (`X_class_latest`) coincida exactamente con el √≠ndice de la variable objetivo (`y_class`), creando `X_class_aligned`. Esto es fundamental para que la divisi√≥n de datos sea correcta.
3.  Divide el conjunto de caracter√≠sticas alineado (`X_class_aligned`) y la variable objetivo binaria (`y_class`) en conjuntos de entrenamiento y prueba. Se utiliza un tama√±o de prueba del 20% y se especifica un `random_state` para reproducibilidad.
4.  Se utiliza `stratify=y_class` durante la divisi√≥n. Esto es muy importante debido al **desbalance de clases** en `y_class`, ya que asegura que la proporci√≥n de pa√≠ses en la clase positiva (reducci√≥n significativa) sea aproximadamente la misma en los conjuntos de entrenamiento y prueba.

El resultado son los conjuntos `X_train_class`, `X_test_class`, `y_train_class` y `y_test_class`, listos para ser utilizados en el entrenamiento y la evaluaci√≥n del modelo de clasificaci√≥n.
"""

from sklearn.model_selection import train_test_split

# Select the latest year's data for each country in X_class_imputed
# Assuming the index is a MultiIndex with (Country, Year)
# We can group by the first level (Country) and select the last entry in each group
X_class_latest = X_class_imputed.groupby(level=0).tail(1)

# Ensure the index of X_class_latest matches the index of y_class
# This is crucial for train_test_split to work correctly
X_class_aligned = X_class_latest.loc[y_class.index]


# Split the data into training and testing sets for classification
X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(
    X_class_aligned, # Use the aligned feature set
    y_class,
    test_size=0.2,
    random_state=42, # Use a random state for reproducibility
    stratify=y_class # Use stratify to maintain the proportion of classes in both train and test sets
)

# Display the shapes of the resulting sets
print("Shape of X_train_class:", X_train_class.shape)
print("Shape of X_test_class:", X_test_class.shape)
print("Shape of y_train_class:", y_test_class.shape) # Corrected typo here
print("Shape of y_test_class:", y_test_class.shape)

# Display the distribution of the target variable in train and test sets
print("\nDistribution of y_train_class:")
display(y_train_class.value_counts(normalize=True))

print("\nDistribution of y_test_class:")
display(y_test_class.value_counts(normalize=True))

"""##Paso 3 : Se entrena el modelo regresi√≥n log√≠stica"""

from sklearn.linear_model import LogisticRegression

# Instantiate a Logistic Regression model
# Use class_weight='balanced' to handle the imbalanced nature of the target variable
logistic_reg_model = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced')

# Train the model using the training data
logistic_reg_model.fit(X_train_class, y_train_class)

print("Modelo de Regresi√≥n Log√≠stica entrenado exitosamente.")

"""## Paso 3 (Continuaci√≥n): Evaluaci√≥n del Rendimiento del Clasificador üìäüìàüìâ

Una vez que el modelo de clasificaci√≥n ha sido entrenado, evaluaremos su rendimiento con el conjunto de prueba para entender que tan bien generaliza a datos no vistos y que tan efectivo es para identificar la clase de inter√©s (pa√≠ses con reducci√≥n significativa de emisiones).

Esta celda de c√≥digo calcula y muestra varias m√©tricas de evaluaci√≥n comunes para modelos de clasificaci√≥n binaria:

*   **Accuracy:** Proporci√≥n de predicciones correctas sobre el total de casos.
*   **Precision:** De todos los casos predichos como positivos, ¬øcu√°ntos son realmente positivos? Es importante cuando el costo de un falso positivo es alto.
*   **Recall (Sensibilidad):** De todos los casos realmente positivos, ¬øcu√°ntos fueron correctamente identificados por el modelo? Es importante cuando el costo de un falso negativo es alto.
*   **F1-Score:** Media arm√≥nica de la precisi√≥n y el recall, √∫til cuando hay un desbalance de clases.
*   **ROC AUC Score:** Mide la capacidad del modelo para distinguir entre las clases positivas y negativas. Un valor m√°s cercano a 1 indica una mejor capacidad de discriminaci√≥n.
*   **Classification Report:** Proporciona un resumen detallado de precisi√≥n, recall y F1-score por clase, junto con el soporte (n√∫mero de instancias en cada clase).

**An√°lisis de los Resultados:**

Al interpretar estas m√©tricas, especialmente en presencia de un desbalance de clases (como vimos con `y_class`), es crucial prestar especial atenci√≥n al **Recall** y al **F1-Score** para la clase minoritaria (la clase positiva, que representa la reducci√≥n significativa de emisiones).

*   Un **alto Recall** para la clase positiva indica que el modelo es bueno identificando a la mayor√≠a de los pa√≠ses que efectivamente lograron una reducci√≥n significativa.
*   Una **buena Precisi√≥n** para la clase positiva indica que cuando el modelo predice que un pa√≠s lograr√° una reducci√≥n, es probable que sea correcto.
*   El **F1-Score** ayuda a encontrar un equilibrio entre precisi√≥n y recall.

El **ROC AUC Score** proporciona una medida general de la capacidad discriminatoria del modelo, independientemente del umbral de clasificaci√≥n.

Analizando la salida de la celda, podemos determinar las fortalezas y debilidades del modelo para predecir pa√≠ses con y sin reducci√≥n significativa de emisiones.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report

# Make predictions on the test set
y_pred_class = logistic_reg_model.predict(X_test_class)

# Predict probabilities on the test set (needed for AUC)
y_pred_proba_class = logistic_reg_model.predict_proba(X_test_class)[:, 1]

# Calculate evaluation metrics
accuracy = accuracy_score(y_test_class, y_pred_class)
precision = precision_score(y_test_class, y_pred_class)
recall = recall_score(y_test_class, y_pred_class)
f1 = f1_score(y_test_class, y_pred_class)
roc_auc = roc_auc_score(y_test_class, y_pred_proba_class)

# Print the evaluation metrics
print("Logistic Regression Classifier Evaluation:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")

# Print the classification report for a more detailed view
print("\nClassification Report:")
print(classification_report(y_test_class, y_pred_class))

"""**An√°lisis Espec√≠fico de los Resultados de Evaluaci√≥n:**

*   **Accuracy (0.8810):** El modelo predijo correctamente la clase (reducci√≥n significativa o no) para aproximadamente el 88.1% de los pa√≠ses en el conjunto de prueba. Si bien esto parece alto, la Accuracy puede ser enga√±osa en datasets desbalanceados, ya que un modelo que simplemente predice la clase mayoritaria para todos los casos podr√≠a tener una alta accuracy.
*   **Precision (0.5833):** De todos los pa√≠ses que el modelo predijo que tendr√≠an una reducci√≥n significativa de emisiones (clase 1), aproximadamente el 58.3% realmente lograron esa reducci√≥n. Esto significa que hay una cantidad notable de "falsos positivos" (pa√≠ses predichos con reducci√≥n que no la tuvieron).
*   **Recall (1.0000):** El modelo identific√≥ correctamente a *todos* los pa√≠ses en el conjunto de prueba que realmente lograron una reducci√≥n significativa de emisiones (clase 1). Un Recall de 1.0000 (o 100%) indica que no hubo "falsos negativos" (pa√≠ses con reducci√≥n que el modelo no identific√≥). Este es un resultado muy fuerte para identificar la clase minoritaria.
*   **F1-Score (0.7368):** El F1-Score es una m√©trica √∫til para datasets desbalanceados, ya que considera tanto la precisi√≥n como el recall. Un F1-Score de 0.7368 para la clase positiva sugiere un equilibrio razonable entre la capacidad del modelo para identificar correctamente a los pa√≠ses que reducen emisiones y la cantidad de falsos positivos.
*   **ROC AUC Score (0.9347):** Un ROC AUC Score de 0.9347 es bastante alto y est√° cerca de 1, lo que indica que el modelo tiene una excelente capacidad para distinguir entre los pa√≠ses que lograr√°n una reducci√≥n significativa de emisiones y los que no, independientemente del umbral de clasificaci√≥n.

**Interpretaci√≥n del Classification Report:**

El reporte por clase refuerza estos puntos:

*   **Clase 0 (No reducci√≥n):** El modelo tiene una precisi√≥n perfecta (1.00) y un recall alto (0.86) para la clase mayoritaria, lo que significa que es muy bueno para identificar a los pa√≠ses que probablemente no lograr√°n una reducci√≥n significativa.
*   **Clase 1 (Reducci√≥n significativa):** Para la clase minoritaria, el recall es de 1.00, lo que es excelente (identifica a todos los pa√≠ses que s√≠ redujeron). La precisi√≥n es de 0.58, lo que indica que, aunque encuentra a todos los positivos, tambi√©n etiqueta algunos negativos como positivos. El F1-score de 0.74 para esta clase es un buen indicador general de rendimiento en la clase de inter√©s.

**Conclusi√≥n General:**

El modelo de Regresi√≥n Log√≠stica, con ajuste de peso de clases, muestra un rendimiento muy prometedor para identificar pa√≠ses que probablemente lograr√°n una reducci√≥n significativa de las emisiones de $\text{CO}_2$. Su alto Recall para la clase positiva es particularmente valioso para identificar a todos los posibles candidatos a la reducci√≥n. Si bien tiene una precisi√≥n moderada para esta clase (lo que implica algunos falsos positivos), su capacidad general de discriminaci√≥n (AUC) es fuerte. Esto lo convierte en una herramienta √∫til para identificar pa√≠ses donde las estrategias de mitigaci√≥n podr√≠an ser m√°s efectivas.

## Paso 3 (Continuaci√≥n): Interpretaci√≥n de los Coeficientes del Modelo de Clasificaci√≥n (Regresi√≥n Log√≠stica) üßê

Para entender que caracter√≠sticas son m√°s importantes para el modelo de Regresi√≥n Log√≠stica al predecir la probabilidad de una reducci√≥n significativa de emisiones, examinamos los coeficientes del modelo.

Los coeficientes nos indican la magnitud y direcci√≥n de la relaci√≥n entre cada variable predictora y el logaritmo de las probabilidades (log-odds) de la clase positiva (reducci√≥n significativa de emisiones), manteniendo otras variables constantes.

*   Un coeficiente positivo sugiere que un aumento en el valor de la caracter√≠stica est√° asociado con un aumento en la probabilidad de reducci√≥n significativa.
*   Un coeficiente negativo sugiere que un aumento en el valor de la caracter√≠stica est√° asociado con una disminuci√≥n en la probabilidad de reducci√≥n significativa.
*   La magnitud absoluta del coeficiente indica la fuerza de esta asociaci√≥n.

Esta celda de c√≥digo extrae los coeficientes del modelo entrenado, los asocia con los nombres de las caracter√≠sticas y los ordena por su valor absoluto para identificar f√°cilmente las variables m√°s influyentes.
"""

# Get the coefficients from the trained Logistic Regression model
classification_coefficients = logistic_reg_model.coef_[0]

# Create a pandas Series to associate coefficients with feature names
classification_coefficients_series = pd.Series(classification_coefficients, index=X_train_class.columns)

# Sort the coefficients by their absolute value to see the most influential features
sorted_classification_coefficients = classification_coefficients_series.abs().sort_values(ascending=False)

# Display the sorted coefficients (showing both positive and negative impacts)
print("Coeficientes del modelo de Regresi√≥n Log√≠stica (ordenados por valor absoluto):")
pd.set_option('display.max_rows', None) # Set option to display all rows
pd.options.display.float_format = '{:.4f}'.format # Set float format to 4 decimal places
display(classification_coefficients_series[sorted_classification_coefficients.index]) # Display coefficients in the order of absolute value
pd.reset_option('display.max_rows') # Reset option to default
pd.reset_option('display.float_format') # Reset float format to default

"""#An√°lisis
Los coeficientes con los mayores valores absolutos son los que tienen la influencia m√°s fuerte en el modelo. Observando la lista que proporcionaste:

Caracter√≠sticas como WB_RISE_CON_AFEL, OWID_CB_PRIMARY_ENERGY_CONSUMPTION, OWID_CB_TOTAL_GHG, y OWID_CB_TOTAL_GHG_EXCLUDING_LUCF tienen coeficientes negativos relativamente grandes en valor absoluto. Esto sugiere que mayores valores en estos indicadores est√°n asociados con una menor probabilidad de lograr una reducci√≥n significativa de emisiones.
Por otro lado, OWID_CB_CUMULATIVE_OIL_CO2 y WB_RISE_RE_HE_CO tienen coeficientes positivos (aunque OWID_CB_CUMULATIVE_OIL_CO2 es relativamente peque√±o en magnitud). Un coeficiente positivo sugiere que mayores valores en estas caracter√≠sticas est√°n asociados con una mayor probabilidad de reducci√≥n significativa de emisiones.
En general, los indicadores relacionados con el consumo total de energ√≠a y las emisiones totales de gases de efecto invernadero (tanto incluyendo como excluyendo el cambio de uso de la tierra) parecen tener una influencia negativa significativa en la probabilidad de reducci√≥n de emisiones, seg√∫n este modelo. Esto es intuitivo, ya que un alto consumo de energ√≠a primaria o altas emisiones totales har√≠an m√°s dif√≠cil lograr una reducci√≥n significativa.

Es importante recordar que esta es una interpretaci√≥n basada en un modelo lineal y que las relaciones reales pueden ser m√°s complejas. Sin embargo, estos coeficientes nos dan una idea de qu√© factores est√°n m√°s fuertemente asociados con la reducci√≥n de emisiones en los datos que utilizamos.

## Paso 3 (Continuaci√≥n): Predicci√≥n de la Probabilidad de Reducci√≥n Significativa por Pa√≠s üìà

Una vez que hemos entrenado e interpretado el modelo de clasificaci√≥n, podemos utilizarlo para predecir la probabilidad de que cada pa√≠s logre una reducci√≥n significativa de las emisiones de $\text{CO}_2$. Esto nos permite identificar los pa√≠ses que, seg√∫n el modelo, tienen una mayor probabilidad de √©xito en la reducci√≥n de emisiones.

Esta celda de c√≥digo realiza lo siguiente:

1.  Utiliza el modelo de Regresi√≥n Log√≠stica entrenado (`logistic_reg_model`) para predecir la probabilidad de la clase positiva (reducci√≥n significativa, representada por 1) para cada pa√≠s en el conjunto de datos alineado (`X_class_aligned`).
2.  Crea una Serie de pandas con estas probabilidades, utilizando el √≠ndice de los datos alineados (pa√≠s y a√±o) para mantener la asociaci√≥n.
3.  Ordena las probabilidades predichas en orden descendente para mostrar primero los pa√≠ses con la mayor probabilidad de lograr una reducci√≥n significativa.

El resultado nos proporciona una lista de pa√≠ses clasificados por su probabilidad predicha de √©xito en la reducci√≥n de emisiones.
"""

# Predict the probability of the positive class (reduction = 1) for each country in the aligned dataset
predicted_probabilities = logistic_reg_model.predict_proba(X_class_aligned)[:, 1]

# Create a pandas Series from the predicted probabilities, using the index of the aligned features (country and year)
predicted_probabilities_series = pd.Series(predicted_probabilities, index=X_class_aligned.index)

# Sort the probabilities in descending order to see which countries have the highest predicted probability of reduction
sorted_predicted_probabilities = predicted_probabilities_series.sort_values(ascending=False)

# Display the top N countries with the highest predicted probability of achieving significant CO2 reduction
# Let's display the top 20 countries as an example
print("Pa√≠ses con la mayor probabilidad predicha de lograr una reducci√≥n significativa de emisiones de CO‚ÇÇ:")
display(sorted_predicted_probabilities.head(20))

"""## Paso 4. Recomendaciones Pol√≠ticas

Bas√°ndose en las caracter√≠sticas comunes identificadas en los pa√≠ses que han logrado una reducci√≥n significativa de las emisiones de $\text{CO}_2$, se pueden derivar las siguientes recomendaciones pol√≠ticas para otros pa√≠ses:

*(Estas recomendaciones se basar√≠an directamente en las caracter√≠sticas comunes identificadas en el paso anterior. Por ejemplo, si la inversi√≥n en renovables fue una caracter√≠stica com√∫n, la recomendaci√≥n ser√≠a invertir en energ√≠as renovables).*

*   **Fomentar la Inversi√≥n en Energ√≠as Limpias:** Implementar incentivos fiscales, subsidios y marcos regulatorios que promuevan la inversi√≥n y el desarrollo de fuentes de energ√≠a renovable (solar, e√≥lica, hidr√°ulica, etc.).
*   **Establecer Mecanismos de Precios al Carbono:** Considerar la implementaci√≥n de impuestos al carbono o sistemas de comercio de emisiones para internalizar los costos ambientales de las emisiones de $\text{CO}_2$ y desincentivar el uso de combustibles f√≥siles.
*   **Promover la Eficiencia Energ√©tica:** Desarrollar e implementar pol√≠ticas y programas que mejoren la eficiencia en el uso de la energ√≠a en todos los sectores (industria, transporte, edificios), reduciendo as√≠ el consumo total de energ√≠a y las emisiones asociadas.
*   **Apoyar la Transici√≥n en el Sector Transporte:** Impulsar la adopci√≥n de veh√≠culos el√©ctricos a trav√©s de incentivos, desarrollo de infraestructura de carga y promoci√≥n del transporte p√∫blico sostenible.
*   **Invertir en Investigaci√≥n y Desarrollo:** Apoyar la innovaci√≥n en tecnolog√≠as bajas en carbono y soluciones de mitigaci√≥n para acelerar la transici√≥n hacia una econom√≠a m√°s verde.
*   **Establecer Metas Claras y Medibles:** Definir objetivos ambiciosos y plazos espec√≠ficos para la reducci√≥n de emisiones, respaldados por mecanismos de seguimiento y reporte transparentes.
*   **Fomentar la Cooperaci√≥n Internacional:** Participar en acuerdos internacionales y compartir conocimientos y mejores pr√°cticas con otros pa√≠ses para acelerar la acci√≥n clim√°tica global.

### Conclusi√≥n

El modelo de clasificaci√≥n ha identificado caracter√≠sticas que est√°n asociadas con la reducci√≥n significativa de las emisiones de $\text{CO}_2$. El an√°lisis de las caracter√≠sticas clave y las caracter√≠sticas comunes de los pa√≠ses clasificados como exitosos proporciona informaci√≥n valiosa para los responsables pol√≠ticos que buscan implementar estrategias efectivas de mitigaci√≥n del cambio clim√°tico. Las recomendaciones pol√≠ticas derivadas de este an√°lisis pueden servir como una gu√≠a para acelerar la transici√≥n hacia un futuro con bajas emisiones de carbono.

Pa√≠ses con la mayor probabilidad predicha de lograr una reducci√≥n significativa de emisiones de CO‚ÇÇ:


1. Germany	2023
2. France	2023
3. United Kingdom	2023
4. Japan	2023
5. United States

'

'

'

#Pregunta 5: An√°lisis estrat√©gico y aplicaci√≥n del modelo (Question 5: Strategic Analysis and Model Application)

Utilizando el modelo predictivo y el clasificador, realice un an√°lisis estrat√©gico para responder: ‚ÄúSi un pa√≠s invirtiera fuertemente en energ√≠as renovables, ¬øcu√°l es la probabilidad de que esta inversi√≥n conduzca a una reducci√≥n de las emisiones de CO2 en los pr√≥ximos cinco a√±os? ¬øC√≥mo deber√≠a este pa√≠s priorizar sus inversiones para maximizar el impacto?‚Äù

##Paso 1 Simulaci√≥n tomando como ejemplo a Alemania
"""

# Select a country and year for the simulation
country_for_simulation = 'Germany'
year_for_simulation = 2023 # Use the latest available year from the data

# Get the data for the selected country and year from X_combined
# Ensure the index is a MultiIndex (Country, Year)
if not isinstance(X_combined.index, pd.MultiIndex):
    X_combined = X_combined.reset_index().set_index(['REF_AREA_LABEL', 'TIME_PERIOD'])

# Get the data for the selected country and year
# Use .copy() to avoid SettingWithCopyWarning
try:
    country_data_for_sim = X_combined.loc[(country_for_simulation, year_for_simulation)].to_frame().T.copy()
except KeyError:
    print(f"Error: Data for {country_for_simulation} in {year_for_simulation} not found in X_combined.")
    # Fallback to the latest available year for the country if the specified year is not found
    latest_year_for_country = X_combined.loc[country_for_simulation].index.get_level_values('TIME_PERIOD').max()
    if pd.notna(latest_year_for_country):
         print(f"Using latest available year for {country_for_simulation}: {latest_year_for_country}")
         country_data_for_sim = X_combined.loc[(country_for_simulation, latest_year_for_country)].to_frame().T.copy()
         year_for_simulation = latest_year_for_country
    else:
         print(f"Error: No data found for {country_for_simulation} in X_combined.")
         country_data_for_sim = None


if country_data_for_sim is not None:
    # Create a copy for the simulated scenario
    country_data_simulated = country_data_for_sim.copy()

    # Define the percentage reduction in fossil fuel emissions to simulate renewable investment
    reduction_percentage = 0.25 # Simulate a 25% reduction as an example

    # Identify columns related to fossil fuel emissions (oil and gas)
    fossil_fuel_cols = [
        'OWID_CB_OIL_CO2',
        'OWID_CB_OIL_CO2_PER_CAPITA',
        'OWID_CB_GAS_CO2',
        'OWID_CB_GAS_CO2_PER_CAPITA',
        # Add other relevant fossil fuel columns if desired, ensuring they are in X_combined
    ]

    # Ensure the identified columns are actually in the simulation data
    fossil_fuel_cols_in_data = [col for col in fossil_fuel_cols if col in country_data_simulated.columns]

    # Simulate the reduction in the identified fossil fuel emission columns
    if fossil_fuel_cols_in_data:
        for col in fossil_fuel_cols_in_data:
            country_data_simulated[col] = country_data_simulated[col] * (1 - reduction_percentage)
        print(f"Simulated a {reduction_percentage*100}% reduction in {fossil_fuel_cols_in_data} for {country_for_simulation} in {year_for_simulation}.")
    else:
        print("Warning: No relevant fossil fuel emission columns found in the data for simulation.")
        # If no fossil fuel columns found, the simulated data will be the same as original data


    # Ensure the columns of the simulation data match the training data columns
    # Select only the columns that were in X_train and in the simulation data
    # Use X_train.columns directly as X_combined has the same columns
    country_data_for_sim_aligned = country_data_for_sim[X_train.columns]
    country_data_simulated_aligned = country_data_simulated[X_train.columns]


    # Use the trained Linear Regression model to predict CO2 emissions for both scenarios
    predicted_co2_original = linear_reg_model.predict(country_data_for_sim_aligned)
    predicted_co2_simulated = linear_reg_model.predict(country_data_simulated_aligned)

    # Calculate the predicted change in CO2 emissions
    predicted_change_co2 = predicted_co2_simulated - predicted_co2_original

    print(f"\nPredicted CO2 emissions for {country_for_simulation} in {year_for_simulation} (Original): {predicted_co2_original[0]:.4f}")
    print(f"Predicted CO2 emissions for {country_for_simulation} in {year_for_simulation} (Simulated Renewable Investment): {predicted_co2_simulated[0]:.4f}")
    print(f"Predicted change in CO2 emissions: {predicted_change_co2[0]:.4f}")

else:
    print("\nCould not perform simulation due to missing country data.")

"""##Paso 1 (Continuaci√≥n) Simulaci√≥n tomando como ejemplo a Alemania"""

# Use the trained Logistic Regression classifier to predict the probability of significant reduction
# We need to use the simulated data with the correct column structure for the classifier

# Get the data for the selected country and year from X_class_aligned
# Use .copy() to avoid SettingWithCopyWarning
try:
    country_data_for_classification_sim = X_class_aligned.loc[(country_for_simulation, year_for_simulation)].to_frame().T.copy()
except KeyError:
     print(f"Error: Data for {country_for_simulation} in {year_for_simulation} not found in X_class_aligned.")
     # If the specific year is not in X_class_aligned (which contains only one year per country, the latest),
     # we should use the data directly from X_class_aligned for that country.
     try:
         country_data_for_classification_sim = X_class_aligned.loc[country_for_simulation].to_frame().T.copy()
         # Update year_for_simulation to the actual year used from X_class_aligned
         year_for_simulation = country_data_for_classification_sim.index.get_level_values('TIME_PERIOD')[0]
         print(f"Using data for {country_for_simulation} from year {year_for_simulation} found in X_class_aligned.")
     except KeyError:
         print(f"Error: Data for {country_for_simulation} not found in X_class_aligned.")
         country_data_for_classification_sim = None


if country_data_for_classification_sim is not None:
    # Create a copy for the simulated scenario for classification
    country_data_simulated_classification = country_data_for_classification_sim.copy()

    # Define the percentage reduction in fossil fuel emissions to simulate renewable investment
    # Use the same reduction percentage as in the predictive model simulation
    reduction_percentage = 0.25

    # Identify columns related to fossil fuel emissions (oil and gas)
    fossil_fuel_cols = [
        'OWID_CB_OIL_CO2',
        'OWID_CB_OIL_CO2_PER_CAPITA',
        'OWID_CB_GAS_CO2',
        'OWID_CB_GAS_CO2_PER_CAPITA',
        # Add other relevant fossil fuel columns if desired, ensuring they are in X_class_aligned
    ]

    # Ensure the identified columns are actually in the simulation data for classification
    fossil_fuel_cols_in_classification_data = [col for col in fossil_fuel_cols if col in country_data_simulated_classification.columns]


    # Simulate the reduction in the identified fossil fuel emission columns in the classification data
    if fossil_fuel_cols_in_classification_data:
        for col in fossil_fuel_cols_in_classification_data:
             country_data_simulated_classification[col] = country_data_simulated_classification[col] * (1 - reduction_percentage)
        print(f"Simulated a {reduction_percentage*100}% reduction in {fossil_fuel_cols_in_classification_data} for classification simulation.")
    else:
         print("Warning: No relevant fossil fuel emission columns found in the classification data for simulation.")


    # Ensure the columns of the simulation data for classification match the training data for classification
    # X_class_aligned should have the same columns as X_train_class
    country_data_simulated_classification_aligned = country_data_simulated_classification[X_train_class.columns]


    # Use the trained Logistic Regression classifier to predict the probability of significant reduction
    predicted_proba_reduction_simulated = logistic_reg_model.predict_proba(country_data_simulated_classification_aligned)[:, 1]

    # Display the predicted probability
    print(f"\nPredicted probability of significant CO2 reduction for {country_for_simulation} in {year_for_simulation} (Simulated Renewable Investment): {predicted_proba_reduction_simulated[0]:.4f}")

    # You can also predict the class (0 or 1) based on the probability and a threshold (default is 0.5)
    predicted_class_simulated = logistic_reg_model.predict(country_data_simulated_classification_aligned)
    print(f"Predicted class (0=no reduction, 1=reduction) for {country_for_simulation} in {year_for_simulation} (Simulated Renewable Investment): {predicted_class_simulated[0]}")

else:
    print("\nCould not perform classification prediction due to missing country data.")

"""## An√°lisis Estrat√©gico y Recomendaciones de Inversi√≥n en Energ√≠as Renovables para Alemania

Este informe presenta un an√°lisis estrat√©gico del impacto potencial de una fuerte inversi√≥n en energ√≠as renovables en las emisiones de $\text{CO}_2$ de Germany, utilizando los modelos predictivo y clasificador desarrollados.

### 1. Simulaci√≥n del Impacto en las Emisiones de $\text{CO}_2$ (Modelo Predictivo)

Para simular una fuerte inversi√≥n en energ√≠as renovables, se aplic√≥ una reducci√≥n del 25.0% en las columnas de emisiones de combustibles f√≥siles relacionadas con el transporte (`OWID_CB_OIL_CO2`, `OWID_CB_OIL_CO2_PER_CAPITA`, `OWID_CB_GAS_CO2`, `OWID_CB_GAS_CO2_PER_CAPITA`) en los datos de Germany para el a√±o 2023.

El modelo de Regresi√≥n Lineal predijo los siguientes resultados de emisiones de $\text{CO}_2$ bajo este escenario simulado:

*   **Emisiones predichas originales:** 595.5538
*   **Emisiones predichas con inversi√≥n simulada:** 578.4780
*   **Cambio predicho en emisiones:** -17.0758

Seg√∫n el modelo predictivo, la simulaci√≥n de una fuerte inversi√≥n en energ√≠as renovables (representada indirectamente por la reducci√≥n en las emisiones de combustibles f√≥siles) resultar√≠a en una **reducci√≥n predicha de 17.0758 unidades** en las emisiones de $\text{CO}_2$ para Germany en el a√±o 2023.

### 2. Probabilidad de Lograr una Reducci√≥n Significativa (Clasificador)

Utilizando el clasificador de Regresi√≥n Log√≠stica con los datos simulados (reflejando la inversi√≥n en energ√≠as renovables), la probabilidad predicha de que Germany logre una reducci√≥n significativa de las emisiones de $\text{CO}_2$ en los pr√≥ximos cinco a√±os es de **1.0000**.

Una probabilidad de 1.0000 (o 100.0%) sugiere que, bajo este escenario de inversi√≥n simulada en energ√≠as renovables, el modelo clasificador predice una **alta probabilidad** de que Germany se encuentre en el grupo de pa√≠ses que logran una reducci√≥n significativa de emisiones. La clase predicha por el modelo fue **1** (donde 1 indica reducci√≥n significativa).

### 3. Priorizaci√≥n de Inversiones y Resultados Esperados

Bas√°ndonos en los an√°lisis previos (correlaci√≥n, modelo predictivo y clasificador) y en la simulaci√≥n realizada, podemos inferir √°reas clave donde la inversi√≥n podr√≠a tener un impacto significativo en la reducci√≥n de emisiones. Si bien no podemos priorizar tipos espec√≠ficos de energ√≠a renovable (solar, e√≥lica, etc.) o regiones sin datos m√°s detallados, podemos priorizar las √°reas tem√°ticas o los indicadores que, seg√∫n nuestros modelos, est√°n m√°s asociados con la reducci√≥n de emisiones.

Considerando los coeficientes de nuestros modelos, las inversiones deber√≠an priorizar √°reas que:

*   Est√©n fuertemente correlacionadas negativamente con las emisiones de $\text{CO}_2$ o positivamente con los indicadores de reducci√≥n de emisiones.
*   Tengan un coeficiente positivo alto en el modelo clasificador (aumentando la probabilidad de reducci√≥n significativa).
*   Correspondan a las variables que al simular su cambio en el modelo predictivo resultaron en la mayor disminuci√≥n de emisiones.

Basado en el an√°lisis de los coeficientes del modelo predictivo y clasificador, las √°reas estrat√©gicas para la inversi√≥n en Germany para maximizar el impacto en la reducci√≥n de emisiones podr√≠an incluir:

*   **Transici√≥n del Sector Energ√©tico:** Invertir en tecnolog√≠as y proyectos que directamente reduzcan la dependencia de combustibles f√≥siles (petr√≥leo, gas, carb√≥n) en la generaci√≥n de energ√≠a y el transporte. Esto se alinea con la simulaci√≥n que realizamos (reducci√≥n de emisiones de petr√≥leo y gas) y con la alta probabilidad de reducci√≥n predicha por el clasificador.
*   **Eficiencia Energ√©tica:** Implementar pol√≠ticas y tecnolog√≠as que mejoren la eficiencia en el uso de la energ√≠a en todos los sectores. Las variables relacionadas con la eficiencia energ√©tica (`OWID_CB_CO2_PER_UNIT_ENERGY`, `OWID_CB_ENERGY_PER_GDP`) tuvieron correlaciones que sugieren que la mejora en la eficiencia puede ser importante.
*   **Pol√≠ticas de Mitigaci√≥n:** Fortalecer e implementar pol√≠ticas activas para la mitigaci√≥n del cambio clim√°tico. Las variables relacionadas con pol√≠ticas en el dataset RISE mostraron asociaciones con la probabilidad de reducci√≥n en el clasificador.
*   **Innovaci√≥n Tecnol√≥gica:** Apoyar la investigaci√≥n y el desarrollo en tecnolog√≠as bajas en carbono.

**Recomendaciones Espec√≠ficas (basadas en el an√°lisis general):**

1.  **Descarbonizaci√≥n del Sector El√©ctrico:** Acelerar la inversi√≥n en fuentes de energ√≠a renovable a gran escala (e√≥lica, solar) y el almacenamiento de energ√≠a para reemplazar la generaci√≥n basada en combustibles f√≥siles.
2.  **Electrificaci√≥n del Transporte:** Incentivar la adopci√≥n de veh√≠culos el√©ctricos y el desarrollo de infraestructura de carga, as√≠ como promover el transporte p√∫blico basado en energ√≠as limpias.
3.  **Mejoras en la Eficiencia Industrial y Residencial:** Implementar est√°ndares de eficiencia, auditor√≠as energ√©ticas e incentivos para la modernizaci√≥n de equipos y edificaciones.
4.  **Fortalecer la Gobernanza y el Marco Regulatorio:** Implementar mecanismos de precios al carbono efectivos y regulaciones claras que incentiven la transici√≥n energ√©tica.

**Resultados Esperados:**

Una inversi√≥n estrat√©gica y fuerte en estas √°reas, seg√∫n lo sugerido por nuestros modelos y an√°lisis, tiene una **alta probabilidad de conducir a una reducci√≥n significativa** de las emisiones de $\text{CO}_2$ en los pr√≥ximos cinco a√±os para Alemania. Los resultados de la simulaci√≥n predijeron una reducci√≥n directa en las emisiones bajo un escenario de menor dependencia de f√≥siles, y el clasificador valid√≥ que este tipo de escenario aumenta significativamente la probabilidad de ser un pa√≠s que logra una reducci√≥n general de emisiones.

### Conclusi√≥n

El an√°lisis estrat√©gico utilizando los modelos predictivo y clasificador indica que una fuerte inversi√≥n en energ√≠as renovables y medidas de eficiencia energ√©tica, respaldadas por pol√≠ticas s√≥lidas, tiene un potencial significativo para reducir las emisiones de $\text{CO}_2$ en Alemania. La priorizaci√≥n de inversiones en la descarbonizaci√≥n del sector energ√©tico y el transporte, junto con mejoras en la eficiencia, son pasos clave para maximizar el impacto y aumentar la probabilidad de lograr una reducci√≥n significativa de emisiones en la pr√≥xima d√©cada.

Hemos combinado los resultados de la simulaci√≥n realizada con el modelo predictivo (que mostr√≥ el impacto de una inversi√≥n en energ√≠as renovables en la cantidad de emisiones de $\text{CO}_2$) con nuestro modelo clasificador.

Utilizando el clasificador entrenado con los datos simulados (representando un escenario de inversi√≥n en energ√≠as renovables), evaluamos la probabilidad de que **Alemania** logre una reducci√≥n significativa de sus emisiones en los pr√≥ximos cinco a√±os.

Seg√∫n este an√°lisis combinado, la probabilidad predicha de una reducci√≥n significativa de $\text{CO}_2$ para Alemania en este escenario simulado es del **100.0%**. Esto sugiere una **alta probabilidad** de que la inversi√≥n en energ√≠as renovables, tal como fue simulada, conduzca a que Alemania se clasifique en el grupo de pa√≠ses que logran reducciones significativas de emisiones.
"""